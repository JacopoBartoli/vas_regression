{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eXXVYXCFKBCJ"
      ],
      "authorship_tag": "ABX9TyO2VNNdEOuRwaQgJugJVWyk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacopoBartoli/vas_regression/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX4rjIu3Kgmd"
      },
      "source": [
        "#1) Install packages and organize imports.\n",
        "\n",
        "In this section we install and import the needed packages. Then we mount our GDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEvE89bU9ZDO"
      },
      "source": [
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CELsJTg9XC2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorboard\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import datetime\n",
        "import math\n",
        "import sklearn.preprocessing\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEl4lSFBK40Y"
      },
      "source": [
        "Useful paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDjB96cE-jz_"
      },
      "source": [
        "# Path to the datasets.\n",
        "DATASET_DIR = '/content/gdrive/My Drive/IVA/data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvRprD2iLcln"
      },
      "source": [
        "Mount the drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBOR2t0YYFUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e37ead3-c984-42c2-b53f-e284b9bec817"
      },
      "source": [
        "# Mount your drive to access the dataset.\n",
        "# Remember to link the dataset as explained above.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpZLk23okxQx"
      },
      "source": [
        "#2) Manage the train set.\n",
        "In this section we manipulate and extract the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9RX5tAUnhGn"
      },
      "source": [
        "##2.1) Load the train set.\n",
        "Define the name of the dataset used for \n",
        "training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJDvwZo7DCvM"
      },
      "source": [
        "# Name of the dataset used.\n",
        "UNSAMPLED_NAME = 'train-velocity-662.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaTqWROxJ2D1"
      },
      "source": [
        "Load the train set from a .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW3SLDKN-NBi"
      },
      "source": [
        "df = pd.read_csv(DATASET_DIR + UNSAMPLED_NAME)\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXXVYXCFKBCJ"
      },
      "source": [
        "## 2.2) Extract the labels from the dataset.\n",
        "\n",
        "First of all we scaled the elements of the dataset. The label were scaled too, they went from the space [0,10] to [0,1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs2TRpy59AOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82aae77-fd1b-4e1f-eacf-f65558108d95"
      },
      "source": [
        "# Define the scaler.\n",
        "# For the label is used a min max scaler.\n",
        "# For the other parameters is used an Standard scaler.\n",
        "min_max_scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1))\n",
        "std_scaler = sklearn.preprocessing.StandardScaler()\n",
        "\n",
        "# Scale the data.\n",
        "print(df.columns)\n",
        "print(df.head())\n",
        "print(list(df))\n",
        "feature_to_scale = list(df.columns)\n",
        "feature_to_scale.remove('Sequenza')\n",
        "feature_to_scale.remove('Frame')\n",
        "feature_to_scale.remove('Label')\n",
        "std_scaler.fit(df[feature_to_scale])\n",
        "df[feature_to_scale] = std_scaler.transform(df[feature_to_scale])\n",
        "\n",
        "#Scale the labels.\n",
        "df[['Label']] = min_max_scaler.fit_transform(df[['Label']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Sequenza', 'Frame', 'Vel0', 'Vel1', 'Vel2', 'Vel3', 'Vel4', 'Vel5',\n",
            "       'Vel6', 'Vel7', 'Vel8', 'Vel9', 'Vel10', 'Vel11', 'Vel12', 'Vel13',\n",
            "       'Vel14', 'Vel15', 'Vel16', 'Vel17', 'Vel18', 'Vel19', 'Vel20', 'Vel21',\n",
            "       'Vel22', 'Vel23', 'Vel24', 'Vel25', 'Vel26', 'Vel27', 'Vel28', 'Vel29',\n",
            "       'Vel30', 'Vel31', 'Vel32', 'Vel33', 'Vel34', 'Vel35', 'Vel36', 'Vel37',\n",
            "       'Vel38', 'Vel39', 'Vel40', 'Vel41', 'Vel42', 'Vel43', 'Vel44', 'Vel45',\n",
            "       'Vel46', 'Vel47', 'Vel48', 'Vel49', 'Vel50', 'Vel51', 'Vel52', 'Vel53',\n",
            "       'Vel54', 'Vel55', 'Vel56', 'Vel57', 'Vel58', 'Vel59', 'Vel60', 'Vel61',\n",
            "       'Vel62', 'Vel63', 'Vel64', 'Vel65', 'Label'],\n",
            "      dtype='object')\n",
            "   Sequenza  Frame      Vel0      Vel1  ...     Vel63     Vel64     Vel65  Label\n",
            "0         0      0  0.070404  0.156512  ...  0.130682  0.264413  0.229226      0\n",
            "1         0      1  0.125513  0.114344  ...  0.120285  0.138199  0.126977      0\n",
            "2         0      2  0.253132  0.231857  ...  0.081278  0.111015  0.104563      0\n",
            "3         0      3  0.187514  0.174368  ...  0.061480  0.069704  0.061332      0\n",
            "4         0      4  0.229464  0.197635  ...  0.051159  0.047535  0.046308      0\n",
            "\n",
            "[5 rows x 69 columns]\n",
            "['Sequenza', 'Frame', 'Vel0', 'Vel1', 'Vel2', 'Vel3', 'Vel4', 'Vel5', 'Vel6', 'Vel7', 'Vel8', 'Vel9', 'Vel10', 'Vel11', 'Vel12', 'Vel13', 'Vel14', 'Vel15', 'Vel16', 'Vel17', 'Vel18', 'Vel19', 'Vel20', 'Vel21', 'Vel22', 'Vel23', 'Vel24', 'Vel25', 'Vel26', 'Vel27', 'Vel28', 'Vel29', 'Vel30', 'Vel31', 'Vel32', 'Vel33', 'Vel34', 'Vel35', 'Vel36', 'Vel37', 'Vel38', 'Vel39', 'Vel40', 'Vel41', 'Vel42', 'Vel43', 'Vel44', 'Vel45', 'Vel46', 'Vel47', 'Vel48', 'Vel49', 'Vel50', 'Vel51', 'Vel52', 'Vel53', 'Vel54', 'Vel55', 'Vel56', 'Vel57', 'Vel58', 'Vel59', 'Vel60', 'Vel61', 'Vel62', 'Vel63', 'Vel64', 'Vel65', 'Label']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw7JfKTI2_7F",
        "outputId": "fdf43074-b94b-41c7-aa61-ea4b0d69fda3"
      },
      "source": [
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Sequenza  Frame      Vel0      Vel1  ...     Vel63     Vel64     Vel65  Label\n",
            "0         0      0 -0.560851 -0.318372  ... -0.067525  0.503878  0.438662    0.0\n",
            "1         0      1 -0.426654 -0.428096  ... -0.117698 -0.059046 -0.070476    0.0\n",
            "2         0      2 -0.115885 -0.122321  ... -0.305934 -0.180287 -0.182083    0.0\n",
            "3         0      3 -0.275672 -0.271911  ... -0.401472 -0.364537 -0.397344    0.0\n",
            "4         0      4 -0.173521 -0.211368  ... -0.451282 -0.463410 -0.472155    0.0\n",
            "\n",
            "[5 rows x 69 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIQPCdPW91sF"
      },
      "source": [
        "## 2.3) Oversampling or Downsampling.\n",
        "\n",
        "Perform the oversampling and downsampling operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dtvmWoy_1AO"
      },
      "source": [
        "# Define some constant needed for the samplig operation.\n",
        "chunk_dim = 230\n",
        "tail_exclusion_percentage = 10\n",
        "\n",
        "num_columns = len(df.columns)\n",
        "# Remove from the count the label column and the frame and sequence indexes.\n",
        "num_features = num_columns - 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdgKhZnLV7qC"
      },
      "source": [
        "We extract the id of each sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFJidI0k9y37"
      },
      "source": [
        "# Extract the sequences number.\n",
        "sequences_number = df['Sequenza'].tolist()\n",
        "sequences_number = list(dict.fromkeys(sequences_number))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45JbfIJcNQEO"
      },
      "source": [
        "Define the utilities functions needed to perform the downsampling and the upsampling operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktyetcy01Pw6"
      },
      "source": [
        "def compute_downsampling(chunk_dim, sequence, padding_item):\n",
        "  seq_length = len(sequence)\n",
        "  change = seq_length % chunk_dim\n",
        "  step = math.floor(seq_length / chunk_dim)\n",
        "  downsampled = [[] for _dummy in range(step)]\n",
        "  if change == 0:\n",
        "    for i in range(step):\n",
        "      for j in range(chunk_dim):\n",
        "        downsampled[i] = np.append(downsampled[i], sequence.iloc[i + j * step])\n",
        "  else:\n",
        "    if change < chunk_dim/2:\n",
        "      # Remove the exceeding elements if they are too few.\n",
        "      offset = change\n",
        "      for i in range(step):\n",
        "        for j in range(chunk_dim):\n",
        "          downsampled[i].append(sequence.iloc[change + i + j * step])\n",
        "    else:\n",
        "      downsampled.append([])\n",
        "      padding = [padding_item for _dummy in range(chunk_dim - change)]\n",
        "      padded_sequence = pd.concat([pd.DataFrame(padding), sequence], ignore_index = True)\n",
        "      for i in range(step + 1 ):\n",
        "        for j in range(chunk_dim):\n",
        "          downsampled[i].append(padded_sequence.iloc[i + j * step])\n",
        "\n",
        "  return downsampled\n",
        "\n",
        "def compute_oversampling(chunk_dim, sequence, padding_item, excluded_percentage):\n",
        "   seq_length = len(sequence)\n",
        "   # Remove the head and the tail of the sequence. Those parts will be excluded\n",
        "   # in the oversampling operation.\n",
        "   excluded_item = math.floor(seq_length / excluded_percentage)\n",
        "   sampling_factor = math.floor(\n",
        "       (chunk_dim - 2 * excluded_item) / (seq_length - 2 * excluded_item))\n",
        "   change = (chunk_dim - 2 * excluded_item) % (seq_length - 2 * excluded_item)\n",
        "\n",
        "   oversampled = []\n",
        "   for i in range(excluded_item):\n",
        "     oversampled.append(sequence.iloc[i])\n",
        "   for i in range(change):\n",
        "     oversampled.append(padding_item)\n",
        "   for i in range(excluded_item, seq_length - (excluded_item)):\n",
        "     for j in range(sampling_factor):\n",
        "       oversampled.append(sequence.iloc[excluded_item + i])\n",
        "   for i in range(excluded_item):\n",
        "     oversampled.append(sequence.iloc[seq_length - (excluded_item) + i])\n",
        "\n",
        "   return oversampled\n",
        "\n",
        "def create_padding_item(sequence, num_features):\n",
        "  result = []\n",
        "  result.append(sequence.iloc[0]['Sequenza'])\n",
        "  result.append(0)\n",
        "  for i in range(num_features):\n",
        "    result.append(0)\n",
        "  result.append(sequence.iloc[0]['Label'])\n",
        "\n",
        "  return pd.Series(result, index=sequence.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7jVjIxAWAWl"
      },
      "source": [
        "Now we iterate over each sequence id, and at each sequence we perform oversampling or undersampling operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNFT6m3z_g_V"
      },
      "source": [
        "# Each element of data is a sampled sequence of frames.\n",
        "data = []\n",
        "new_seq_number = 0\n",
        "for id in tqdm(sequences_number):\n",
        "  sequence = df.loc[df['Sequenza'] == id]\n",
        "  padding_item = create_padding_item(sequence, num_features)\n",
        "  if (len(sequence) > chunk_dim):\n",
        "    sampled = copy.deepcopy(compute_downsampling(chunk_dim, sequence, padding_item))\n",
        "    for i in range(len(sampled)):\n",
        "      for j in range(chunk_dim):\n",
        "        sampled[i][j][0] = new_seq_number\n",
        "      new_seq_number +=1\n",
        "    for seq in sampled:\n",
        "      for item in seq:\n",
        "        data.append(item)\n",
        "    \n",
        "  elif (len(sequence) < chunk_dim):\n",
        "    sampled = copy.deepcopy(compute_oversampling(chunk_dim, sequence, padding_item, tail_exclusion_percentage))\n",
        "    for i in range(len(sampled)):\n",
        "      sampled[i][0] = new_seq_number\n",
        "    new_seq_number +=1\n",
        "    for item in sampled:\n",
        "      data.append(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMQhA8OsJ3fo"
      },
      "source": [
        "# They are pandas Series so we need to concatenate.\n",
        "sampled_df = pd.concat(data, axis=1).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr_glRIDTEt2"
      },
      "source": [
        "print(sampled_df.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77j6R62FT2s9"
      },
      "source": [
        "SAMPLED_NAME = UNSAMPLED_NAME.replace('.csv','-sampled.csv')\n",
        "sampled_df.to_csv(DATASET_DIR + SAMPLED_NAME, index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3DPRJh885IS"
      },
      "source": [
        "#3) Manage the test set.\n",
        "In this section we manipulate and extract the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se4zQRHY85IZ"
      },
      "source": [
        "##3.1) Load the test set.\n",
        "Define the name of the dataset used for \n",
        "test.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CBjDUUp85Ia"
      },
      "source": [
        "# Name of the dataset used.\n",
        "UNSAMPLED_NAME = 'test-velocity-66.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuD5Lx7k85Ia"
      },
      "source": [
        "Load the test set from a .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQfFET7_85Ia"
      },
      "source": [
        "df = pd.read_csv(DATASET_DIR + UNSAMPLED_NAME)\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq-2dxRd85Ib"
      },
      "source": [
        "## 3.2) Extract the labels from the dataset.\n",
        "\n",
        "In this section we scaled the data too. The data now are scaled using the StandardScaler() fitted on the train data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O35l2bS85Ib"
      },
      "source": [
        "# Define the scaler.\n",
        "# Scale the data.\n",
        "feature_to_scale = list(df.columns)\n",
        "feature_to_scale.remove('Sequenza')\n",
        "feature_to_scale.remove('Frame')\n",
        "feature_to_scale.remove('Label')\n",
        "df[feature_to_scale] = std_scaler.transform(df[feature_to_scale])\n",
        "\n",
        "# Scale the labels.\n",
        "df[['Label']] = min_max_scaler.fit_transform(df[['Label']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP_l-7a585Ib"
      },
      "source": [
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESNU8KII85Ib"
      },
      "source": [
        "## 3.3) Oversampling or Downsampling.\n",
        "\n",
        "Perform oversampling and downsampling operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuR1ZaYN85Ic"
      },
      "source": [
        "# Define some constant needed for the samplig operation.\n",
        "chunk_dim = 230\n",
        "# Percentage of samples exluded from oversamplig on the head and the tail of the\n",
        "# sequences.\n",
        "tail_exclusion_percentage = 10\n",
        "\n",
        "num_columns = len(df.columns)\n",
        "# Remove from the count the label column and the frame and sequence indexes.\n",
        "num_features = num_columns - 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w15bGA7iWil_"
      },
      "source": [
        "We extract the id of each sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IbTfXT785Ic"
      },
      "source": [
        "# Extract the sequences number.\n",
        "sequences_number = df['Sequenza'].tolist()\n",
        "sequences_number = list(dict.fromkeys(sequences_number))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds1iwjonVGPC"
      },
      "source": [
        "We use the utilities functions defined above to perform the operation of downsampling and upsampling operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHf-XWzA85Ic"
      },
      "source": [
        "# Each element of data is a sampled sequence of frames.\n",
        "data = []\n",
        "new_seq_number = 0\n",
        "df_copied = df.copy()\n",
        "for id in tqdm(sequences_number):\n",
        "  sequence = df_copied.loc[df_copied.Sequenza == id].copy()\n",
        "  padding_item = create_padding_item(sequence, num_features)\n",
        "  if (len(sequence) > chunk_dim):\n",
        "    sampled = copy.deepcopy(compute_downsampling(chunk_dim, sequence, padding_item))\n",
        "    for i in range(len(sampled)):\n",
        "      for j in range(chunk_dim):\n",
        "        sampled[i][j][0] = new_seq_number\n",
        "      new_seq_number +=1\n",
        "    for seq in sampled:\n",
        "      for item in seq:\n",
        "        data.append(item)\n",
        "    \n",
        "  elif (len(sequence) < chunk_dim):\n",
        "    sampled = copy.deepcopy(compute_oversampling(chunk_dim, sequence, padding_item, tail_exclusion_percentage))\n",
        "    for i in range(len(sampled)):\n",
        "      sampled[i][0] = new_seq_number\n",
        "    new_seq_number +=1\n",
        "    for item in sampled:\n",
        "      data.append(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDDbgQLY85Id"
      },
      "source": [
        "# They are pandas Series so we need to concatenate.\n",
        "sampled_df = pd.concat(data, axis=1).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL1Vgkmq85Ie"
      },
      "source": [
        "print(sampled_df.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfO87PJO85If"
      },
      "source": [
        "SAMPLED_NAME = UNSAMPLED_NAME.replace('.csv','-sampled.csv')\n",
        "sampled_df.to_csv(DATASET_DIR + SAMPLED_NAME, index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}