{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3mr9qPI0vSKU",
        "CmDLWWbEXxPA",
        "kfyIhFnIYUWo"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacopoBartoli/vas_regression/blob/main/Transformer_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mr9qPI0vSKU"
      },
      "source": [
        "#1) Install packages and organize imports.\n",
        "In this section we install the needed packages and import them.\n",
        "We set some variables for the used paths, and mount GDrive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRCPE_6yYdDg",
        "outputId": "3214d515-3d97-4cb6-90eb-0963c85110d4"
      },
      "source": [
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrMRVkjJE88R"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorboard\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import datetime\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xugDyHkCZlbm"
      },
      "source": [
        "DATASET_DIR = '/content/gdrive/My Drive/IVA/data/'\n",
        "LOGS_DIR = '/content/gdrive/My Drive/IVA/logs'\n",
        "CHECKPOINT_DIR = '/content/gdrive/My Drive/IVA/checkpoint/train'\n",
        "MODEL_DIR = '/content/gdrive/My Drive/IVA/model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_tfWIAAxaPB"
      },
      "source": [
        "Mount the drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUIi6rICZcI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bf30f2-1335-44c3-8f70-5c7274eae7a7"
      },
      "source": [
        "# Mount your drive to access the dataset.\n",
        "# Remember to link the dataset as explained above.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls -l \"/content/gdrive/My Drive/\"\n",
        "#!rm -rf './IVA/logs/gradient_tape/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "ls: '/content/gdrive/My Drive/DeepFashion2 Dataset': No such file or directory\n",
            "total 2794\n",
            "drwx------ 2 root root   4096 Mar 18 14:03  3D_result\n",
            "-rw------- 1 root root 555998 Dec  3  2020  7036362.pdf\n",
            "-rw------- 1 root root   6420 Dec  2  2019 '7036362-sol (1).pdf'\n",
            "-rw------- 1 root root   6669 Dec  3  2020  7036362-sol.pdf\n",
            "-rw------- 1 root root 281590 Dec  8  2020  bartoli_jacopo_report2.pdf\n",
            "drwx------ 2 root root   4096 Nov 28  2020 'Colab Notebooks'\n",
            "drwx------ 2 root root   4096 Dec  1  2020  DDM\n",
            "lrw------- 1 root root      0 Mar 24 15:34 'DeepFashion2 Dataset' -> '/content/gdrive/.shortcut-targets-by-id/125F48fsMBz2EF0Cpqk6aaHet5VH399Ok/DeepFashion2 Dataset'\n",
            "drwx------ 6 root root   4096 Sep  3 11:19  IVA\n",
            "-rw------- 1 root root 109584 Nov 10  2020  Jacopo_Bartoli_report.pdf\n",
            "lrw------- 1 root root     25 Nov 30  2020 'My Drive' -> '/content/gdrive/My Drive/'\n",
            "-rw------- 1 root root  94655 Dec  2  2019  PdS-magistrale-ambiti-2019-Bartoli-Jacopo-7036362.pdf\n",
            "-rw------- 1 root root  94655 Dec  2  2019  PdS-magistrale-ambiti-2019-Bartoli-Jacopo.pdf\n",
            "-rw------- 1 root root 442582 Mar 22 17:52  scan.png\n",
            "-rw------- 1 root root 734000 Mar 22  2019  tesi_bartoli_annotata_1.pdf\n",
            "-rw------- 1 root root 511579 Dec  2  2019  variazione_piano_di_studio_7036362.pdf\n",
            "drwx------ 2 root root   4096 Mar 24 21:43  VMR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmDLWWbEXxPA"
      },
      "source": [
        "#2) Define our transformer.\n",
        "In this section we implement our transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfyIhFnIYUWo"
      },
      "source": [
        "##2.1) Utility functions.\n",
        "Define some utilities functions, for the positional encodings and the feed forward network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqAs6b2vX9_a"
      },
      "source": [
        "# Define the positional encoding function.\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7H8TDWjBGKj"
      },
      "source": [
        "# Define the feed forward network\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "                              tf.keras.layers.Dense(dff, activation='relu'),\n",
        "                              tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcnuMP6rY-x4"
      },
      "source": [
        "##2.2) Define the encoder layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J15HQutS_-2T"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(num_heads, output_shape=d_model, key_dim=24)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    attn_output = self.mha(x,x,x,mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdmfhgDYZl84"
      },
      "source": [
        "##2.3) Define the encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQX5vJBXvOdl"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate ) for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef8kHLd9x6xN"
      },
      "source": [
        "## 2.4) Define the transformer with the regression layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oND3EggGFYGa"
      },
      "source": [
        "class EncoderRegressor(tf.keras.Model):\n",
        "  def __init__(self, feat_dim, max_len, d_model, n_heads, num_layers, dim_feedforward, num_classes=1, dropout=0.1, pos_encoding='fixed', activation='gelu', norm='BatchNorm'):\n",
        "    super(EncoderRegressor, self).__init__()\n",
        "\n",
        "    self.max_len = max_len\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    \n",
        "    self.flatten_inp = tf.keras.layers.Flatten()\n",
        "\n",
        "    self.project_inp = tf.keras.layers.Dense(max_len*d_model)\n",
        "\n",
        "    self.reshape = tf.keras.layers.Reshape((max_len,d_model))\n",
        "\n",
        "    self.pos_encoding = positional_encoding(2048, self.d_model)\n",
        "\n",
        "\n",
        "    self.encoder_layer = Encoder(num_layers = num_layers, d_model = d_model, num_heads = n_heads, dff= dim_feedforward, rate=0.01)\n",
        "\n",
        "    self.act = tf.keras.activations.get(activation)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    \n",
        "    self.reg = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    self.feat_dim = feat_dim\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  def call(self, inputs, training):    \n",
        "      enc_padding_mask = None #create_padding_mask(inputs)\n",
        "      seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "      # Flatten the input tensor and map in a different vector space(d_model)\n",
        "      x = self.flatten_inp(inputs)\n",
        "      x = self.project_inp(x)\n",
        "      # Reshape the tensor to adapt the shape [batch_size, sequence_lenght, d_model]\n",
        "      x = self.reshape(x)\n",
        "\n",
        "      # Positional encoding.\n",
        "      x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "      x += self.pos_encoding[:, :seq_len, :]\n",
        "      \n",
        "      # Encoder Layer\n",
        "      x = self.encoder_layer(x, training, enc_padding_mask)\n",
        "      x = self.dropout(x, training=training)\n",
        "\n",
        "      # Regression Layers\n",
        "      x = self.flatten(x)\n",
        "      x = self.reg(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpZLk23okxQx"
      },
      "source": [
        "#3) Manage the data.\n",
        "In this section we manipulate and extract the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9RX5tAUnhGn"
      },
      "source": [
        "##3.1) Load the test set.\n",
        "Define the name of the dataset used for testing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJDvwZo7DCvM"
      },
      "source": [
        "# Name of the dataset used.\n",
        "TEST_SET = 'test.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX-IPSMKxrZR"
      },
      "source": [
        "Load the test set from a .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZU7MgkFx9ty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e813dff3-9430-48bd-bee7-89bdfe56cb39"
      },
      "source": [
        "df = pd.read_csv(DATASET_DIR + TEST_SET)\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Sequenza  Frame      Vel1      Vel2  ...     Vel10     Vel11     Vel12  Label\n",
            "0       180      0  0.085440  0.080623  ...  0.094340  0.050000  0.098995      0\n",
            "1       180      1  0.098489  0.120000  ...  0.036056  0.067082  0.082462      0\n",
            "2       180      2  0.200000  0.170000  ...  0.277308  0.245153  0.053852      0\n",
            "3       180      3  0.078102  0.092195  ...  0.092195  0.120416  0.050000      0\n",
            "4       180      4  0.182483  0.194165  ...  0.053852  0.086023  0.067082      0\n",
            "\n",
            "[5 rows x 15 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXXVYXCFKBCJ"
      },
      "source": [
        "## 3.2) Extract the labels from the dataset.\n",
        "Extract the test set column that contains the labels and group them by sequence id. In this way the output is a list of a single label for each sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTkQINQUyu3u"
      },
      "source": [
        "# Extract the column that contains the test labels.\n",
        "lbl = df['Label']\n",
        "seq_ids = df['Sequenza']\n",
        "\n",
        "temp = pd.concat([seq_ids, lbl], axis=1)\n",
        "temp = temp.set_index('Sequenza')\n",
        "temp = temp.groupby(level='Sequenza').mean()\n",
        "\n",
        "lbl_test = temp['Label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJEegOzWKphi"
      },
      "source": [
        "Remove the label column from the dataframe, and transform the data in the correct input format. The sequences need to be numbered from 0 to $num\\_seq - 1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19PkuhgsJTiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c070a858-b78d-4180-df53-99ad16d4c1c2"
      },
      "source": [
        "# Drop the label column.\n",
        "data = df.drop(['Label'], axis = 1)\n",
        "min_seq = data['Sequenza'].min()\n",
        "num_seqs = data['Sequenza'].max() - data['Sequenza'].min() + 1\n",
        "\n",
        "# Create the new dataset.\n",
        "temp = []\n",
        "for id in tqdm(range(min_seq, min_seq + num_seqs)):\n",
        "  # Extract sequences one by one.\n",
        "  seq = data.loc[data['Sequenza'] == id]\n",
        "\n",
        "  # Remove the unused columns.\n",
        "  seq = seq.drop(['Sequenza','Frame'], axis=1)\n",
        "  num_col = len(seq.columns)\n",
        "\n",
        "  # Iterate over each row of the selected sequence  \n",
        "  temp_row = []\n",
        "  for index, row in seq.iterrows():\n",
        "    temp_row = np.append(temp_row, row)\n",
        "  temp_row = np.reshape(temp_row, (-1, num_col))\n",
        "  # add padding mas\n",
        "\n",
        "  temp.append(temp_row[:])\n",
        "\n",
        "data = temp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 50.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWHhPxz86t4c"
      },
      "source": [
        "data = tf.keras.preprocessing.sequence.pad_sequences(data, maxlen=681, dtype='float64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MpOI7Cfs9We"
      },
      "source": [
        "#print(len(data[100]))\n",
        "#print(len(data[160]))\n",
        "#print(data[100][675])\n",
        "#print(data[160][641])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xw2jkzMK4eO"
      },
      "source": [
        "## 3.3) Create and manage the test set.\n",
        "Transform the dataset ina tensorflow.data.Dataset format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwpIhNo_v8ZI"
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices((data, lbl_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43Zci5c-uVOu"
      },
      "source": [
        "Define batch size and other variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUBcOl2JuUOi"
      },
      "source": [
        "BATCH_SIZE = 1\n",
        "BUFFER_SIZE = 180\n",
        "random_seed = 1337"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1oankNfuZUP"
      },
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE,seed=random_seed)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .prefetch(tf.data.AUTOTUNE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmrpLHucubo0"
      },
      "source": [
        "test_batches = make_batches(ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckT5WkCUpnEA"
      },
      "source": [
        "#4) Evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsWHKn-ivw5v"
      },
      "source": [
        "## 4.1) Set the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRVZLF5ETubo"
      },
      "source": [
        "# Model hyperparameters\n",
        "d_model = 32\n",
        "dim_feedforward = 256\n",
        "n_heads = 6\n",
        "num_layers = 3\n",
        "feat_dim = 12 # Number of feature inside each item of the sequence.\n",
        "max_len=  681 # Lenght of each sequence.\n",
        "\n",
        "# Parameter needed for separate classification from regression.\n",
        "# For now just regression is implemented.\n",
        "num_classes = 1\n",
        "is_classification = False\n",
        "\n",
        "\n",
        "# Network hyperparameter\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "# tfa.optimizers.RectifiedAdam() seems to not work properly.\n",
        "\n",
        "# This loss and accuracy objects are meant for regression.\n",
        "# For classifications other metrics will be needed.\n",
        "accuracy_object = tf.keras.metrics.MeanAbsoluteError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLeUdXBQToDq"
      },
      "source": [
        "## 4.2) Load the model.\n",
        "Since it's a custom model it can't be load as .h5 config. We need to load the weight of the last epoch of the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcGjJXlI68jH",
        "outputId": "901d29b4-5ccf-4946-de3f-0abff0e0ad1c"
      },
      "source": [
        "!ls -l '/content/gdrive/My Drive/IVA/model'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwx------ 4 root root 4096 Sep 16 09:08 20210916-081824\n",
            "drwx------ 4 root root 4096 Sep 16 09:03 20210916-081824_Transformers.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfzEYlhr2nDz"
      },
      "source": [
        "# In this case we use the model associated to the train of the 2021-09-15 at 20:22:03\n",
        "current_time = datetime.datetime(2021, 9, 16, 8, 18, 24).strftime(\"%Y%m%d-%H%M%S\")\n",
        "#transformer = tf.saved_model.load(MODEL_DIR + '/' + current_time + '/Transformer.h5/assets')\n",
        "transformer = tf.saved_model.load('/content/gdrive/My Drive/IVA/model/20210916-081824/transformers')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOcdDMFpvaLW"
      },
      "source": [
        "# Create the transformer.\n",
        "transformer = EncoderRegressor(d_model=d_model, dim_feedforward=dim_feedforward, n_heads=n_heads, num_layers=num_layers, feat_dim=feat_dim, max_len=max_len, num_classes = num_classes)\n",
        "\n",
        "# Checkpoint management.\n",
        "ckpt = tf.train.Checkpoint(trasformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_DIR, max_to_keep=5)\n",
        "print(ckpt_manager.latest_checkpoint)\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print(\"Restored latest checkpoint\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3pxHFQVFwdd"
      },
      "source": [
        "## 4.2) Custom implementation of the accuracy function. \n",
        "Add a way to customize the accuracy function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YInErpodyKFT"
      },
      "source": [
        "def accuracy_function(real, pred):\n",
        "\n",
        "  accuracies = accuracy_object(real, pred)\n",
        "  \n",
        "  return accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTr2DpFCY9Ft"
      },
      "source": [
        "Create the metrics object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anzv8mRpISiF"
      },
      "source": [
        "test_accuracy = tf.keras.metrics.Mean(name='test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvhdmvOAyjZ3"
      },
      "source": [
        "## 4.3) Set the paths for Tensorboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txv-PSFPY1w6"
      },
      "source": [
        "The test_log_dir need to be associate to a valid train_log_dir using their timestamp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk3_uzV8Y06P"
      },
      "source": [
        "# In this case we use the model associated to the train of the 2021-09-15 at 20:22:03\n",
        "current_time = datetime.datetime(2021, 9, 16, 8, 18, 24).strftime(\"%Y%m%d-%H%M%S\")\n",
        "test_log_dir = LOGS_DIR + '/gradient_tape/' + current_time + '/test'\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud5JnnWNGUk6"
      },
      "source": [
        "## 4.2) Evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3c-T4EY0NQ9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "e4ea625b-25f4-49a7-d859-52a46976f093"
      },
      "source": [
        "start = time.time()\n",
        "  \n",
        "test_accuracy.reset_states()\n",
        "\n",
        "# Needed for histogram visualization.\n",
        "predictions_histogram = []\n",
        "labels_histogram = []\n",
        "\n",
        "\n",
        "for (batch, (inp, tar)) in enumerate(test_batches):\n",
        "    predictions = transformer(inp, training = False)\n",
        "    \n",
        "    accuracy = accuracy_function(tar, predictions)\n",
        "    test_accuracy(accuracy)\n",
        "    # Save the histogram of predictions.\n",
        "    predictions_histogram = np.hstack((predictions_histogram, tf.reshape(predictions, len(predictions))))    \n",
        "    labels_histogram = np.hstack((labels_histogram, tar))\n",
        "with test_summary_writer.as_default():\n",
        "   tf.summary.scalar('accuracy', test_accuracy.result(),step = 0)\n",
        "   tf.summary.histogram('predictions distribution', predictions_histogram, step=0)\n",
        "   tf.summary.histogram('label distribution', labels_histogram, step=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-e85c7b56b486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '_UserObject' object is not callable"
          ]
        }
      ]
    }
  ]
}