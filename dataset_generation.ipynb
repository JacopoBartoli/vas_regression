{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataset_generation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"n-kblIM6FlLu"},"source":["# 1) Organize imports"]},{"cell_type":"markdown","metadata":{"id":"NG0t8yE7Fx6y"},"source":["In this section we install and import the needed packages. Then we mount our GDrive."]},{"cell_type":"code","metadata":{"id":"W6zDrfY1Ft7D"},"source":["import os\n","import pickle\n","import matplotlib\n","import operator\n","import statistics\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xG7eghoXGLTL"},"source":["Useful paths."]},{"cell_type":"code","metadata":{"id":"R6EYvyL7hMMV"},"source":["# Path of two part of dataset\n","COORD_DF_PATH = '/content/gdrive/My Drive/IVA/Datasets/info/2d_skeletal_data_unbc_coords.csv'\n","SEQ_DF_PATH = '/content/gdrive/My Drive/IVA/Datasets/info/2d_skeletal_data_unbc_sequence.csv'\n","# Path where save the data extract from dataset\n","FIG_DIR = '/content/gdrive/My Drive/IVA/Datasets/info/histogram.png'\n","DATASET_DIR = '/content/gdrive/My Drive/IVA/Datasets/info/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Ds2WVcXG-r7"},"source":["Mount the drive."]},{"cell_type":"code","metadata":{"id":"3iVdUcC2WwC4"},"source":["# Mount your drive to access the dataset.\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tLc0XPJkaIZl"},"source":["# 2) Information on dataset distribution"]},{"cell_type":"markdown","metadata":{"id":"bn_hD43xhR02"},"source":["Save and show some information of dataset and histogram with all the lengths of the sequences."]},{"cell_type":"code","metadata":{"id":"GO4kePhWWamk"},"source":["data = pd.read_csv(SEQ_DF_PATH)\n","\n","mean = data['num_frames'].mean()\n","max = data['num_frames'].max()\n","min = data['num_frames'].min()\n","\n","data['num_frames'].plot(kind='hist',bins=200)\n","plt.axvline(data['num_frames'].mean(), c='red')\n","plt.xlabel('Number of Frame')\n","plt.ylabel('Frequencies')\n","plt.title(\"sequence length distribution\")\n","plt.savefig(FIG_DIR, dpi=200)\n","plt.close()\n","\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vhxlwxkw0HUU"},"source":["# 3) Selection of Landmarks "]},{"cell_type":"markdown","metadata":{"id":"Lavwd1yh9Kd8"},"source":["Possible landmarks selection:\n","\n","*   Eyes *→* [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n","*   Eyebrows *→* [17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n","*   Nose *→* [27, 28, 29, 30, 31, 32, 33, 34, 35]\n","*   Mouth *→* [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,64, 65]\n","* Face countours *→* [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ArMkSQpqA_Gd"},"source":["# Some examples of landmark selection \n","\n","# Eye\n","group_eye = [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n","\n","# Eyebrows\n","group_eyebrows = [17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n","\n","# Nose\n","group_nose = [27, 28, 29, 30, 31, 32, 33, 34, 35]\n","\n","# Mouth\n","group_mouth = [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65]\n","\n","# Face countours\n","group_face_countours = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n","\n","# Eyes + Eyebrows\n","group_eyes_eyebrows = [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n","\n","# Nose + Mouth\n","group_nose_mouth = [27, 28, 29, 30, 31, 32, 33, 34, 35, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65]\n","\n","# All\n","group_all = range(0, 66)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q6OkGS9WCIFM"},"source":["Select the group of the landmarks to be used."]},{"cell_type":"code","metadata":{"id":"VXrBqz9aCOTz"},"source":["selected_lndks_idx = group_all"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Se5tzoXCRrL"},"source":["Show the landmarks selected of a frame example."]},{"cell_type":"code","metadata":{"id":"xzjLPK_t0aqO"},"source":["seq_name = \"['bn080t1aeunaff']\"\n","\n","coord_df = pd.read_csv(COORD_DF_PATH)\n","seq_df = pd.read_csv(SEQ_DF_PATH)\n","\n","seq = seq_df.query('sequence_name== @seq_name')\n","seq_idx = seq.index.values[0]\n","\n","VAS = seq['VAS'][seq_idx]\n","num_frames = seq['num_frames'][seq_idx]\n","\n","print(\"Sequence: \" + seq_name + \" --> VAS: \" + str(VAS) + \" - Frame numbers: \" + str(num_frames))\n","lndks = coord_df.loc[coord_df['0'] == seq_idx].values\n","\n","num_lndks = 66\n","lndks = lndks[:, 2:]\n","\n","lndks_x = lndks[:, :num_lndks]\n","lndks_y = lndks[:, num_lndks:]\n","\n","selected_x = []\n","selected_y = []\n","lndks_selected_x = []\n","lndks_selected_y = []\n","for n in range(0,num_frames):\n","    for i in selected_lndks_idx:\n","        selected_x.append(lndks_x[n][i])\n","        selected_y.append(lndks_y[n][i])\n","    lndks_selected_x.append(selected_x)\n","    lndks_selected_y.append(selected_y)\n","    selected_x = []\n","    selected_y = []\n","\n","lndks_selected_x = np.array(lndks_selected_x)\n","lndks_selected_y = np.array(lndks_selected_y)\n","\n","\n","plt.title(\"Landmarks Frame \" + str(0))\n","plt.scatter(lndks_selected_x[0, :], lndks_selected_y[0, :], s=100, alpha=0.5)\n","plt.xlim(50,200)\n","plt.ylim(75,225)\n","plt.gca().invert_yaxis()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CLnqvBFgFUPB"},"source":["# 4) Dataset Generation\n"]},{"cell_type":"markdown","metadata":{"id":"jqP6PMFPIPAT"},"source":["In this section we extract the data from dataset."]},{"cell_type":"markdown","metadata":{"id":"5iuv3YetjTHF"},"source":["##4.1) Utility functions\n"]},{"cell_type":"markdown","metadata":{"id":"yIZjllqEIVrj"},"source":["Define some utilities functions."]},{"cell_type":"code","metadata":{"id":"X8Eh19CXFXwz"},"source":["# Get the velocities of all selected landmark for each frame of each sequence\n","\n","def get_velocities_frames():\n","  \n","  coord_df = pd.read_csv(COORD_DF_PATH)\n","  seq_df = pd.read_csv(SEQ_DF_PATH)\n","  velocities = []\n","  for seq_num in np.arange(seq_df.shape[0]):\n","      lndks = coord_df.loc[coord_df['0'] == seq_num].values\n","      lndks = lndks[:, 2:]\n","      num_lndks = 66\n","      num_frames = seq_df['num_frames'][seq_num]\n","      centroid_x = np.array([np.sum(lndks[i, 0:num_lndks]) / num_lndks for i in range(num_frames)])\n","      centroid_y = np.array([np.sum(lndks[i, num_lndks:]) / num_lndks for i in range(num_frames)])\n","\n","      offset = np.hstack((np.repeat(centroid_x.reshape(-1, 1), num_lndks, axis=1),\n","                          np.repeat(centroid_y.reshape(-1, 1), num_lndks, axis=1)))\n","\n","      lndks_centered = lndks - offset\n","\n","      lndks_centered[:, 30] = centroid_x\n","      lndks_centered[:, 30 + num_lndks] = centroid_y\n","\n","      lndk_vel = np.power(np.power(lndks_centered[0:lndks_centered.shape[0] - 1, 0:num_lndks] -\n","                                  lndks_centered[1:lndks_centered.shape[0], 0:num_lndks], 2) +\n","                          np.power(lndks_centered[0:lndks_centered.shape[0] - 1, num_lndks:] -\n","                                  lndks_centered[1:lndks_centered.shape[0], num_lndks:], 2), 0.5)\n","      data_velocities = []\n","      for k in np.arange(1, lndk_vel.shape[0]):\n","          data_velocities.append(np.array(lndk_vel[k, selected_lndks_idx]))\n","      velocities.append(np.array(data_velocities))\n","      \n","  return velocities"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NGqOaDN5NPaf"},"source":["##4.2) Dataset Generation"]},{"cell_type":"code","metadata":{"id":"3ISew7wfGf7x"},"source":["# Create two csv files, one for the training dataset and one for the test dataset\n","\n","velocities = get_velocities_frames()\n","seq_df = pd.read_csv(SEQ_DF_PATH)\n","\n","lst = []\n","element = []\n","sequenza = []\n","for id_seq in range(0, len(velocities)):\n","    vas = seq_df.iloc[id_seq][1]\n","    element.append(id_seq)\n","    sequenza = velocities[id_seq]\n","    for id_frames in range(0, len(sequenza)):\n","        element.append(id_frames)\n","        frame = sequenza[id_frames]\n","        for v in range(0, len(frame)):\n","            velocita = frame[v]\n","            element.append(velocita)\n","        element.append(vas)\n","        lst.append(element)\n","        element = [id_seq]\n","    element = []\n","\n","col = ['Sequenza','Frame']\n","for i in range(0, len(selected_lndks_idx)):\n","    s = 'Vel' + str(i)\n","    col.append(s)\n","col.append('Label')\n","\n","df = pd.DataFrame(lst,columns=col)\n","\n","train = df.loc[(df['Sequenza'] < 180)]\n","test = df.loc[(df['Sequenza'] >= 180)]\n","\n","name_csv_train = DATASET_DIR + 'train-velocity-' + str(len(selected_lndks_idx)) + '.csv'\n","name_csv_test = DATASET_DIR + 'test-velocity-' + str(len(selected_lndks_idx)) + '.csv'\n","\n","train.to_csv(name_csv_train, index=False)\n","test.to_csv(name_csv_test, index=False)\n","\n","\n","    "],"execution_count":null,"outputs":[]}]}
