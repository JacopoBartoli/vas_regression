{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_train_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CmDLWWbEXxPA",
        "kfyIhFnIYUWo",
        "hcnuMP6rY-x4"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacopoBartoli/vas_regression/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mr9qPI0vSKU"
      },
      "source": [
        "#1) Install packages and organize imports.\n",
        "In this section we install the needed packages and import them.\n",
        "We set some variables for the used paths, and mount GDrive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRCPE_6yYdDg",
        "outputId": "cb1be81f-28c7-42e1-d4fb-393fa06ffb35"
      },
      "source": [
        "!pip install tensorflow-addons"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrMRVkjJE88R"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorboard\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import datetime\n",
        "import math\n",
        "import sklearn.preprocessing"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfeT4MTUYo-Y"
      },
      "source": [
        "Save some useful paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xugDyHkCZlbm"
      },
      "source": [
        "DATASET_DIR = '/content/gdrive/My Drive/IVA/data/'\n",
        "LOGS_DIR = '/content/gdrive/My Drive/IVA/logs'\n",
        "CHECKPOINT_DIR = '/content/gdrive/My Drive/IVA/checkpoint/train'\n",
        "MODEL_DIR = '/content/gdrive/My Drive/IVA/model'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_tfWIAAxaPB"
      },
      "source": [
        "Mount the drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUIi6rICZcI8"
      },
      "source": [
        "# Mount your drive to access the dataset.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#!ls -l \"/content/gdrive/My Drive/\"\n",
        "#!rm -rf './IVA/logs/gradient_tape/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmDLWWbEXxPA"
      },
      "source": [
        "#2) Define our transformer.\n",
        "In this section we implement our transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfyIhFnIYUWo"
      },
      "source": [
        "##2.1) Utility functions.\n",
        "Define some utilities functions, for the positional encodings and the feed forward network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqAs6b2vX9_a"
      },
      "source": [
        "# Define the positional encoding function.\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7H8TDWjBGKj"
      },
      "source": [
        "# Define the feed forward network\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "                              tf.keras.layers.Dense(dff, activation='relu'),\n",
        "                              tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcnuMP6rY-x4"
      },
      "source": [
        "##2.2) Define the encoder layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J15HQutS_-2T"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(num_heads, output_shape=d_model, key_dim=24)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    attn_output = self.mha(x,x,x,mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdmfhgDYZl84"
      },
      "source": [
        "##2.3) Define the encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQX5vJBXvOdl"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate ) for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef8kHLd9x6xN"
      },
      "source": [
        "## 2.4) Define the transformer with the LSTM  layer.\n",
        "\n",
        "In this section we define our model using the layers defined above. Only the encoder part of a transformed model is used. The output of this encoder is feeded to a LSTM that perform regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oND3EggGFYGa"
      },
      "source": [
        "class EncoderRegressor(tf.keras.Model):\n",
        "  def __init__(self, feat_dim, max_len, d_model, n_heads, num_layers, dim_feedforward, num_classes=1, dropout=0.1, pos_encoding='fixed', activation='sigmoid', norm='BatchNorm'):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.max_len = max_len\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    \n",
        "    self.flatten_inp = tf.keras.layers.Flatten()\n",
        "\n",
        "    self.project_inp = tf.keras.layers.Dense(max_len*d_model)\n",
        "\n",
        "    self.reshape = tf.keras.layers.Reshape((max_len, d_model))\n",
        "\n",
        "    self.pos_encoding = positional_encoding(2048, self.d_model)\n",
        "\n",
        "\n",
        "    self.encoder_layer = Encoder(num_layers = num_layers, d_model = d_model, num_heads = n_heads, dff= dim_feedforward, rate=0.01)\n",
        "\n",
        "    self.act = tf.keras.activations.get(activation)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    self.lstm = tf.keras.layers.LSTM(num_classes, activation=self.act)\n",
        "\n",
        "    self.feat_dim = feat_dim\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  def call(self, inputs, training):    \n",
        "      enc_padding_mask = None #create_padding_mask(inputs)\n",
        "      seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "      # Flatten the input tensor and map in a different vector space(d_model)\n",
        "      x = self.flatten_inp(inputs)\n",
        "      x = self.project_inp(x)\n",
        "\n",
        "      # Reshape the tensor to adapt the shape [batch_size, sequence_lenght, d_model]\n",
        "      x = self.reshape(x)\n",
        "\n",
        "      # Positional encoding.\n",
        "      x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "      x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "      \n",
        "      # Encoder Layer\n",
        "      x = self.encoder_layer(x, training, enc_padding_mask)\n",
        "      x = self.dropout(x, training=training)\n",
        "\n",
        "      # LSTM\n",
        "      x = self.lstm(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpZLk23okxQx"
      },
      "source": [
        "#3) Manage the data.\n",
        "In this section we manipulate and extract the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9RX5tAUnhGn"
      },
      "source": [
        "##3.1) Load the train set.\n",
        "\n",
        "Define the name of the dataset used for \n",
        "training.\n",
        "\n",
        "The data in the .csv can have a variable number of features. But three column are always needed. They are 'Sequenza', 'Frame' and 'Label'.\n",
        "The first represent the id of a sequence, the second the id of a frame. The third represent the label of each frame.\n",
        "\n",
        "\n",
        "Each row of the file need to represent a frame, and each frame of the same sequence need to have the same label.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJDvwZo7DCvM"
      },
      "source": [
        "# Name of the dataset used.\n",
        "TRAIN_SET = 'train-velocity-66-sampled.csv'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaTqWROxJ2D1"
      },
      "source": [
        "Load the train set from a .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW3SLDKN-NBi",
        "outputId": "4a3260d4-5358-47e8-8a89-8fcd9b429fd0"
      },
      "source": [
        "df = pd.read_csv(DATASET_DIR + TRAIN_SET)\n",
        "print(df.head())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Sequenza  Frame      Vel0      Vel1  ...     Vel63     Vel64     Vel65  Label\n",
            "0       0.0    0.0  0.005976  0.015146  ...  0.027066  0.050636  0.048805    0.0\n",
            "1       0.0    1.0  0.010653  0.011065  ...  0.024913  0.026466  0.027035    0.0\n",
            "2       0.0    2.0  0.021485  0.022437  ...  0.016834  0.021260  0.022263    0.0\n",
            "3       0.0    3.0  0.015915  0.016874  ...  0.012734  0.013349  0.013058    0.0\n",
            "4       0.0    4.0  0.019476  0.019125  ...  0.010596  0.009103  0.009860    0.0\n",
            "\n",
            "[5 rows x 69 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXXVYXCFKBCJ"
      },
      "source": [
        "## 3.2) Divide the data in train and validation set.\n",
        "\n",
        "The data is splitted in train and validation set. After that we divide the label from the features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC-wuiZBobvZ"
      },
      "source": [
        "# Dimension of the validation set.\n",
        "VAL_PERCENTAGE = 10\n",
        "valid_dim = math.floor(df['Sequenza'].max()*VAL_PERCENTAGE / 100)\n",
        "train_dim = df['Sequenza'].max() - valid_dim\n",
        "max_seq = df['Sequenza'].max()\n",
        "\n",
        "\n",
        "# Divide by train and validation set.\n",
        "df_valid = df.loc[df['Sequenza'] >= (max_seq - valid_dim)]\n",
        "df_train = df.loc[df['Sequenza'] < train_dim]\n",
        "df_valid = df_valid.drop(['Frame'], axis=1)\n",
        "df_train = df_train.drop(['Frame'], axis=1)\n",
        "# Extract the labels.\n",
        "lbl_valid = df_valid['Label']\n",
        "lbl_train = df_train['Label']\n",
        "# Remove the labels from the data.\n",
        "df_valid = df_valid.drop(['Label'], axis = 1)\n",
        "df_train = df_train.drop(['Label'], axis = 1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnUzxRESX5qi",
        "outputId": "ac5442f2-d430-43ce-ed7f-522169925027"
      },
      "source": [
        "print(df_train.head())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Sequenza      Vel0      Vel1  ...     Vel63     Vel64     Vel65\n",
            "0       0.0  0.005976  0.015146  ...  0.027066  0.050636  0.048805\n",
            "1       0.0  0.010653  0.011065  ...  0.024913  0.026466  0.027035\n",
            "2       0.0  0.021485  0.022437  ...  0.016834  0.021260  0.022263\n",
            "3       0.0  0.015915  0.016874  ...  0.012734  0.013349  0.013058\n",
            "4       0.0  0.019476  0.019125  ...  0.010596  0.009103  0.009860\n",
            "\n",
            "[5 rows x 67 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMlSIkZdTM17"
      },
      "source": [
        "##3.3) Prerocessing of the sequences.\n",
        "\n",
        "In the dataset each row represent a frame of the sequence. Each frame in a sequence has the same label. We want to make some preprocessing for having a dataset that has a single label for each sequence (not one for each frame). We want that each item of the dataset represent a whole sequence and not a frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp4TI_UTwStp"
      },
      "source": [
        "# Need to pass different in a separate ways lbl and data.\n",
        "def preprocessing_sequences(data, lbl):\n",
        "  # Preprocess the labels.\n",
        "  # The label and the ids of the sequence are concatenated together.\n",
        "  seq_ids = data['Sequenza']\n",
        "\n",
        "  tmp = pd.concat([seq_ids, lbl], axis=1)\n",
        "  tmp = tmp.set_index('Sequenza')\n",
        "  # Then they are gourped by sequence id so we can have a single label for each\n",
        "  # sequence.\n",
        "  tmp = tmp.groupby(level='Sequenza').mean()\n",
        "\n",
        "  labels = tmp['Label'].values\n",
        "\n",
        "  min_seq = data['Sequenza'].min()\n",
        "  num_seqs = data['Sequenza'].max() - data['Sequenza'].min() + 1\n",
        "  min_seq = int(min_seq)\n",
        "  num_seqs = int(num_seqs)\n",
        "\n",
        "  # Create the new dataset.\n",
        "  temp = []\n",
        "  for id in tqdm(range(min_seq, min_seq + num_seqs)):\n",
        "    # Extract sequences one by one.\n",
        "    seq = data.loc[data['Sequenza'] == id]\n",
        "\n",
        "    # Remove the unused columns.\n",
        "    seq = seq.drop(['Sequenza'], axis=1)\n",
        "    num_col = len(seq.columns)\n",
        "\n",
        "    # Iterate over each row of the selected sequence  \n",
        "    temp_row = []\n",
        "    for index, row in seq.iterrows():\n",
        "      temp_row = np.append(temp_row, row)\n",
        "    temp_row = np.reshape(temp_row, (-1, num_col))\n",
        "\n",
        "    temp.append(temp_row[:])\n",
        "\n",
        "  return temp, labels\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J8MI_pQGT4j",
        "outputId": "92bc3c67-b030-481b-e39d-1ce8c359a9c8"
      },
      "source": [
        "df_train, lbl_train = preprocessing_sequences(df_train, lbl_train)\n",
        "df_valid, lbl_valid = preprocessing_sequences(df_valid, lbl_valid)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:03<00:00, 49.95it/s]\n",
            "100%|██████████| 22/22 [00:00<00:00, 47.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VvZMY3A1k3B"
      },
      "source": [
        "## 3.3) Create and manage the train and validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwpIhNo_v8ZI"
      },
      "source": [
        "ds_train = tf.data.Dataset.from_tensor_slices((df_train, lbl_train))\n",
        "ds_valid = tf.data.Dataset.from_tensor_slices((df_valid, lbl_valid))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzAf41xJu1PL"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "BUFFER_SIZE = 5000\n",
        "random_seed = 1337"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmmKbrPZYibk"
      },
      "source": [
        "Function to apply some preprocessing when making batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bh-ogBsyccq"
      },
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE,seed=random_seed)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .prefetch(tf.data.AUTOTUNE))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yug1V-wIpTqm"
      },
      "source": [
        "Now we divide in batches the validation and training sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYlfnP6jyfoR"
      },
      "source": [
        "train_batches = make_batches(ds_train)\n",
        "val_batches = make_batches(ds_valid)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckT5WkCUpnEA"
      },
      "source": [
        "#4) Training Phase.\n",
        "\n",
        "In this section we organize all the operation needed to perform the train of the model and evaluate its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLeUdXBQToDq"
      },
      "source": [
        "## 4.1)Set the hyperparameters.\n",
        "Set the transformer hyperparameter, define the learning rate, optimizer and loss type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRVZLF5ETubo"
      },
      "source": [
        "# Model hyperparameters\n",
        "d_model = 128 # Dimension of the hidden representation.\n",
        "dim_feedforward = 256\n",
        "n_heads = 6\n",
        "num_layers = 3\n",
        "feat_dim = len(df.columns) - 3 # Number of feature inside each item of the sequence.\n",
        "\n",
        "# Changed.\n",
        "max_len=  len(df_train[0]) # Lenght of each sequence.\n",
        "\n",
        "# Parameter needed for separate classification from regression.\n",
        "# For now just regression is implemented.\n",
        "num_classes = 1\n",
        "is_classification = False\n",
        "\n",
        "\n",
        "# Network hyperparameter\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "# tfa.optimizers.RectifiedAdam() seems to not work properly.\n",
        "\n",
        "# This loss and accuracy objects are meant for regression.\n",
        "# For classifications other metrics will be needed.\n",
        "loss_object = tf.keras.losses.MeanSquaredError()\n",
        "accuracy_object = tf.keras.metrics.MeanAbsoluteError()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3pxHFQVFwdd"
      },
      "source": [
        "## 4.2) Custom implementation of the loss and accuracy functions.\n",
        "\n",
        "Add a way to customize the loss and accuracy functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YInErpodyKFT"
      },
      "source": [
        "def loss_function(real,pred):\n",
        "\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  return loss_\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "\n",
        "  accuracies = accuracy_object(real, pred)\n",
        "  \n",
        "  return accuracies"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTr2DpFCY9Ft"
      },
      "source": [
        "Create the metric objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anzv8mRpISiF"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "val_accuracy = tf.keras.metrics.Mean(name='validion_accuracy')\n",
        "val_loss = tf.keras.metrics.Mean(name='validation_loss')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_y-xc4IzDgu"
      },
      "source": [
        "## 4.4) Manage checkpoint and Tensorboard.\n",
        "Create the model and load last checkpoint if it exist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRgByXvatFtu"
      },
      "source": [
        "# Create the transformer.\n",
        "transformer = EncoderRegressor(d_model=d_model, dim_feedforward=dim_feedforward, n_heads=n_heads, num_layers=num_layers, feat_dim=feat_dim, max_len=max_len, num_classes = num_classes)\n",
        "\n",
        "# Checkpoint management.\n",
        "use_checkpoint = False\n",
        "if use_checkpoint:\n",
        "  ckpt = tf.train.Checkpoint(trasformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "  ckpt_manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_DIR, max_to_keep=5)\n",
        "  if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Restored latest checkpoint\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txv-PSFPY1w6"
      },
      "source": [
        "Set paths for tensorboard visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk3_uzV8Y06P"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = LOGS_DIR + '/gradient_tape/' + current_time + '/train'\n",
        "valid_log_dir = LOGS_DIR + '/gradient_tape/' + current_time + '/valid'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "valid_summary_writer = tf.summary.create_file_writer(valid_log_dir)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud5JnnWNGUk6"
      },
      "source": [
        "## 4.5) Train the model.\n",
        "Set the number of epoch and define the train step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkrNOWVTz_94"
      },
      "source": [
        "# Number of epochs\n",
        "EPOCHS = 30"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHlsTQSds8eF"
      },
      "source": [
        "def valid_step(inp,tar):\n",
        "  tar_real = tar\n",
        "\n",
        "  \n",
        "  predictions = transformer(inp, training = False) \n",
        "  loss = loss_function(tar_real, predictions)\n",
        "  accuracy = accuracy_function(tar_real, predictions)\n",
        "\n",
        "\n",
        "  val_loss(loss)\n",
        "  val_accuracy(accuracy)\n",
        "  \n",
        "  return predictions\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCoA_OUOYnlP"
      },
      "source": [
        "def train_step(inp,tar):\n",
        "  tar_real = tar\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = transformer(inp, training = True) \n",
        "    loss = loss_function(tar_real, predictions)\n",
        "    accuracy = accuracy_function(tar_real, predictions)\n",
        "    \n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(accuracy)\n",
        "  \n",
        "  return predictions"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXFVuSG0YFhv"
      },
      "source": [
        "Start the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4n-a9GeZXLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b9f0fec-0a9b-407f-e194-00b540490949"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  # Needed for histogram visualization.\n",
        "  predictions_histogram = []\n",
        "  labels_histogram = []\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "    predictions = train_step(inp, tar)\n",
        "    # Save the histogram of predictions.\n",
        "    predictions_histogram = np.hstack((predictions_histogram, tf.reshape(predictions, len(predictions))))    \n",
        "    labels_histogram = np.hstack((labels_histogram, tar))\n",
        "\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    tf.summary.histogram('predictions distribution', predictions_histogram, step = epoch)\n",
        "    tf.summary.histogram('ground truth distribution', labels_histogram, step = epoch)\n",
        "  \n",
        "  predictions_histogram = []\n",
        "  labels_histogram = []\n",
        "  val_loss.reset_states()\n",
        "  val_accuracy.reset_states()\n",
        "\n",
        "  for (batch, (inp,tar)) in enumerate(val_batches):\n",
        "    predictions = valid_step(inp,tar)\n",
        "    # Save the histogram of predictions.\n",
        "    predictions_histogram = np.hstack((predictions_histogram, tf.reshape(predictions, len(predictions))))    \n",
        "    labels_histogram = np.hstack((labels_histogram, tar))\n",
        "\n",
        "  with valid_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', val_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', val_accuracy.result(), step=epoch)\n",
        "    tf.summary.histogram('predictions distribution', predictions_histogram, step = epoch)\n",
        "    tf.summary.histogram('ground truth distribution', labels_histogram, step = epoch)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  if (epoch + 1) % 5 == 0 and use_checkpoint:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n",
        "\n",
        "if use_checkpoint:\n",
        "  print(ckpt_manager.save())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss 0.1303 Accuracy 0.3056\n",
            "Time taken for 1 epoch: 84.12 secs\n",
            "\n",
            "Epoch 2 Loss 0.1015 Accuracy 0.2961\n",
            "Time taken for 1 epoch: 73.17 secs\n",
            "\n",
            "Epoch 3 Loss 0.0981 Accuracy 0.2802\n",
            "Time taken for 1 epoch: 73.01 secs\n",
            "\n",
            "Epoch 4 Loss 0.1033 Accuracy 0.2791\n",
            "Time taken for 1 epoch: 84.10 secs\n",
            "\n",
            "Epoch 5 Loss 0.0977 Accuracy 0.2791\n",
            "Time taken for 1 epoch: 84.48 secs\n",
            "\n",
            "Epoch 6 Loss 0.0986 Accuracy 0.2750\n",
            "Time taken for 1 epoch: 84.12 secs\n",
            "\n",
            "Epoch 7 Loss 0.1029 Accuracy 0.2761\n",
            "Time taken for 1 epoch: 84.48 secs\n",
            "\n",
            "Epoch 8 Loss 0.1036 Accuracy 0.2771\n",
            "Time taken for 1 epoch: 84.48 secs\n",
            "\n",
            "Epoch 9 Loss 0.1026 Accuracy 0.2776\n",
            "Time taken for 1 epoch: 73.27 secs\n",
            "\n",
            "Epoch 10 Loss 0.1026 Accuracy 0.2785\n",
            "Time taken for 1 epoch: 84.09 secs\n",
            "\n",
            "Epoch 11 Loss 0.0946 Accuracy 0.2778\n",
            "Time taken for 1 epoch: 84.11 secs\n",
            "\n",
            "Epoch 12 Loss 0.0885 Accuracy 0.2764\n",
            "Time taken for 1 epoch: 73.10 secs\n",
            "\n",
            "Epoch 13 Loss 0.0828 Accuracy 0.2733\n",
            "Time taken for 1 epoch: 72.73 secs\n",
            "\n",
            "Epoch 14 Loss 0.0915 Accuracy 0.2717\n",
            "Time taken for 1 epoch: 84.09 secs\n",
            "\n",
            "Epoch 15 Loss 0.0866 Accuracy 0.2704\n",
            "Time taken for 1 epoch: 84.48 secs\n",
            "\n",
            "Epoch 16 Loss 0.0846 Accuracy 0.2685\n",
            "Time taken for 1 epoch: 72.30 secs\n",
            "\n",
            "Epoch 17 Loss 0.0810 Accuracy 0.2672\n",
            "Time taken for 1 epoch: 84.48 secs\n",
            "\n",
            "Epoch 18 Loss 0.0787 Accuracy 0.2645\n",
            "Time taken for 1 epoch: 84.48 secs\n",
            "\n",
            "Epoch 19 Loss 0.0816 Accuracy 0.2637\n",
            "Time taken for 1 epoch: 71.68 secs\n",
            "\n",
            "Epoch 20 Loss 0.0813 Accuracy 0.2619\n",
            "Time taken for 1 epoch: 71.83 secs\n",
            "\n",
            "Epoch 21 Loss 0.0802 Accuracy 0.2606\n",
            "Time taken for 1 epoch: 84.48 secs\n",
            "\n",
            "Epoch 22 Loss 0.0832 Accuracy 0.2592\n",
            "Time taken for 1 epoch: 84.11 secs\n",
            "\n",
            "Epoch 23 Loss 0.0836 Accuracy 0.2588\n",
            "Time taken for 1 epoch: 84.48 secs\n",
            "\n",
            "Epoch 24 Loss 0.0800 Accuracy 0.2579\n",
            "Time taken for 1 epoch: 72.28 secs\n",
            "\n",
            "Epoch 25 Loss 0.0687 Accuracy 0.2561\n",
            "Time taken for 1 epoch: 72.11 secs\n",
            "\n",
            "Epoch 26 Loss 0.0712 Accuracy 0.2550\n",
            "Time taken for 1 epoch: 84.09 secs\n",
            "\n",
            "Epoch 27 Loss 0.0626 Accuracy 0.2532\n",
            "Time taken for 1 epoch: 84.48 secs\n",
            "\n",
            "Epoch 28 Loss 0.0744 Accuracy 0.2518\n",
            "Time taken for 1 epoch: 84.48 secs\n",
            "\n",
            "Epoch 29 Loss 0.0662 Accuracy 0.2503\n",
            "Time taken for 1 epoch: 71.45 secs\n",
            "\n",
            "Epoch 30 Loss 0.0708 Accuracy 0.2490\n",
            "Time taken for 1 epoch: 71.78 secs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTYGPGv7YXTD"
      },
      "source": [
        "## 4.6) Save the model.\n",
        "Save the summary of the model on tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sWNU6wWqf7D"
      },
      "source": [
        "def get_summary_str(model):\n",
        "    lines = []\n",
        "    model.summary(print_fn=lines.append)\n",
        "    # Add initial spaces to avoid markdown formatting in TensorBoard\n",
        "    return '    ' + '\\n    '.join(lines)\n",
        "\n",
        "# Add the summary as text in Tensorboard\n",
        "with train_summary_writer.as_default():\n",
        "  tf.summary.text('Model configuration', get_summary_str(transformer), step=0)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6djv_C7RypI1"
      },
      "source": [
        "class ExportTransformer(tf.Module):\n",
        "  def __init__(self, transformer):\n",
        "    self.transformer = transformer\n",
        "    \n",
        "  @tf.function()\n",
        "  def __call__(self, inputs):\n",
        "    result = self.transformer(inputs, training = False)\n",
        "\n",
        "    return result"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRWfGyznVOTG"
      },
      "source": [
        "Save the transformer model in .h5 format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DObxkZ5uwhUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95b9ffc-efdb-4a76-a052-412a8f021d70"
      },
      "source": [
        "# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\n",
        "# Currently not working\n",
        "exporter = ExportTransformer(transformer)\n",
        "tf.saved_model.save(exporter, export_dir=MODEL_DIR + '/' + current_time + '/transformers')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as __call__, dropout_6_layer_call_fn, dropout_6_layer_call_and_return_conditional_losses, encoder_layer_layer_call_fn, encoder_layer_layer_call_and_return_conditional_losses while saving (showing 5 of 186). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/IVA/model/20211011-214154/transformers/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/IVA/model/20211011-214154/transformers/assets\n"
          ]
        }
      ]
    }
  ]
}