{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3mr9qPI0vSKU",
        "hpZLk23okxQx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacopoBartoli/vas_regression/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mr9qPI0vSKU"
      },
      "source": [
        "#1) Install packages and organize imports.\n",
        "In this section we install the needed packages and import them.\n",
        "We set some variables for the used paths, and mount GDrive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRCPE_6yYdDg"
      },
      "source": [
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrMRVkjJE88R"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorboard\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time, datetime, math, io, sklearn.preprocessing, itertools\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfeT4MTUYo-Y"
      },
      "source": [
        "Save some useful paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xugDyHkCZlbm"
      },
      "source": [
        "# Path to the datasets.\n",
        "DATASET_DIR = '/content/gdrive/My Drive/IVA/data/'\n",
        "# Path to where we save logs.\n",
        "LOGS_DIR = '/content/gdrive/My Drive/IVA/logs'\n",
        "# Path to where we save the checkpoints.\n",
        "CHECKPOINT_DIR = '/content/gdrive/My Drive/IVA/checkpoint/train'\n",
        "# Path to where we save the model.\n",
        "MODEL_DIR = '/content/gdrive/My Drive/IVA/model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_tfWIAAxaPB"
      },
      "source": [
        "Mount the drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUIi6rICZcI8"
      },
      "source": [
        "# Mount your drive to access the dataset.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmDLWWbEXxPA"
      },
      "source": [
        "#2) Define our transformer.\n",
        "In this section we implement our transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfyIhFnIYUWo"
      },
      "source": [
        "##2.1) Utility functions.\n",
        "Define some utilities functions, for the positional encodings and the feed forward network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqAs6b2vX9_a"
      },
      "source": [
        "# Define the positional encoding function.\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7H8TDWjBGKj"
      },
      "source": [
        "# Define the feed forward network\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "                              tf.keras.layers.Dense(dff, activation='relu'),\n",
        "                              tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcnuMP6rY-x4"
      },
      "source": [
        "##2.2) Define the encoder layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J15HQutS_-2T"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(num_heads, output_shape=d_model, key_dim=24)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    attn_output = self.mha(x,x,x,mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdmfhgDYZl84"
      },
      "source": [
        "##2.3) Define the encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQX5vJBXvOdl"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate ) for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef8kHLd9x6xN"
      },
      "source": [
        "## 2.4) Define the transformer with the LSTM  layer.\n",
        "\n",
        "In this section we define our model using the layers defined above. Only the encoder part of a transformed model is used. The output of this encoder is feeded to a LSTM that perform regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oND3EggGFYGa"
      },
      "source": [
        "class EncoderRegressor(tf.keras.Model):\n",
        "  def __init__(self, feat_dim, max_len, d_model, n_heads, num_layers, dim_feedforward, num_classes=1, dropout=0.1, pos_encoding='fixed', activation='sigmoid'):\n",
        "\n",
        "    super(EncoderRegressor, self).__init__()\n",
        "\n",
        "    self.max_len = max_len\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    \n",
        "    self.flatten_inp = tf.keras.layers.Flatten()\n",
        "\n",
        "    self.project_inp = tf.keras.layers.Dense(max_len*d_model)\n",
        "\n",
        "    self.reshape = tf.keras.layers.Reshape((max_len, d_model))\n",
        "\n",
        "    self.pos_encoding = positional_encoding(2048, self.d_model)\n",
        "\n",
        "\n",
        "    self.encoder_layer = Encoder(num_layers = num_layers, d_model = d_model, num_heads = n_heads, dff= dim_feedforward, rate=0.01)\n",
        "\n",
        "    self.act = tf.keras.activations.get(activation)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    self.lstm = tf.keras.layers.LSTM(num_classes, activation=self.act)\n",
        "\n",
        "    self.feat_dim = feat_dim\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  def call(self, inputs, training):    \n",
        "      enc_padding_mask = None\n",
        "      seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "      # Flatten the input tensor and map in a different vector space(d_model)\n",
        "      x = self.flatten_inp(inputs)\n",
        "      x = self.project_inp(x)\n",
        "\n",
        "      # Reshape the tensor to adapt the shape [batch_size, sequence_lenght, d_model]\n",
        "      x = self.reshape(x)\n",
        "\n",
        "      # Positional encoding.\n",
        "      x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "      x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "      \n",
        "      # Encoder Layer\n",
        "      x = self.encoder_layer(x, training, enc_padding_mask)\n",
        "      x = self.dropout(x, training=training)\n",
        "\n",
        "      # LSTM\n",
        "      x = self.lstm(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpZLk23okxQx"
      },
      "source": [
        "#3) Manage the data.\n",
        "In this section we manipulate and extract the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9RX5tAUnhGn"
      },
      "source": [
        "##3.1) Load the train set.\n",
        "\n",
        "Define the name of the dataset used for \n",
        "training.\n",
        "\n",
        "The data in the .csv can have a variable number of features. But three column are always needed. They are 'Sequenza', 'Frame' and 'Label'.\n",
        "The first represent the id of a sequence, the second the id of a frame. The third represent the label of each frame.\n",
        "\n",
        "\n",
        "Each row of the file need to represent a frame, and each frame of the same sequence need to have the same label.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJDvwZo7DCvM"
      },
      "source": [
        "# Name of the dataset used.\n",
        "TRAIN_SET = 'train-velocity-66-sampled.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaTqWROxJ2D1"
      },
      "source": [
        "Load the train set from a .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW3SLDKN-NBi"
      },
      "source": [
        "df = pd.read_csv(DATASET_DIR + TRAIN_SET)\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXXVYXCFKBCJ"
      },
      "source": [
        "## 3.2) Divide the data in train and validation set.\n",
        "\n",
        "The data is splitted in train and validation set. After that we divide the label from the features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC-wuiZBobvZ"
      },
      "source": [
        "# Dimension of the validation set.\n",
        "VAL_PERCENTAGE = 10\n",
        "valid_dim = math.floor(df['Sequenza'].max()*VAL_PERCENTAGE / 100)\n",
        "train_dim = df['Sequenza'].max() - valid_dim\n",
        "max_seq = df['Sequenza'].max()\n",
        "\n",
        "# Needed later for confusion matrix\n",
        "number_of_labels = df['Label'].tolist()\n",
        "number_of_labels = len(list(dict.fromkeys(number_of_labels))) - 1\n",
        "\n",
        "\n",
        "# Divide by train and validation set.\n",
        "df_valid = df.loc[df['Sequenza'] >= (max_seq - valid_dim)]\n",
        "df_train = df.loc[df['Sequenza'] < train_dim]\n",
        "df_valid = df_valid.drop(['Frame'], axis=1)\n",
        "df_train = df_train.drop(['Frame'], axis=1)\n",
        "\n",
        "# Extract the labels.\n",
        "lbl_valid = df_valid['Label']\n",
        "lbl_train = df_train['Label']\n",
        "\n",
        "\n",
        "# Remove the labels from the data.\n",
        "df_valid = df_valid.drop(['Label'], axis = 1)\n",
        "df_train = df_train.drop(['Label'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnUzxRESX5qi"
      },
      "source": [
        "print(df_train.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMlSIkZdTM17"
      },
      "source": [
        "##3.3) Preprocessing of the sequences.\n",
        "\n",
        "In the dataset each row represent a frame of the sequence. Each frame in a sequence has the same label. We want to make some preprocessing for having a dataset that has a single label for each sequence (not one for each frame). We want that each item of the dataset represent a whole sequence and not a frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp4TI_UTwStp"
      },
      "source": [
        "# Need to pass different in a separate ways lbl and data.\n",
        "def preprocessing_sequences(data, lbl):\n",
        "  # Preprocess the labels.\n",
        "  # The label and the ids of the sequence are concatenated together.\n",
        "  seq_ids = data['Sequenza']\n",
        "\n",
        "  tmp = pd.concat([seq_ids, lbl], axis=1)\n",
        "  tmp = tmp.set_index('Sequenza')\n",
        "  # Then they are gourped by sequence id so we can have a single label for each\n",
        "  # sequence.\n",
        "  tmp = tmp.groupby(level='Sequenza').mean()\n",
        "\n",
        "  labels = tmp['Label'].values\n",
        "\n",
        "  min_seq = data['Sequenza'].min()\n",
        "  num_seqs = data['Sequenza'].max() - data['Sequenza'].min() + 1\n",
        "  min_seq = int(min_seq)\n",
        "  num_seqs = int(num_seqs)\n",
        "\n",
        "  # Create the new dataset.\n",
        "  temp = []\n",
        "  for id in tqdm(range(min_seq, min_seq + num_seqs)):\n",
        "    # Extract sequences one by one.\n",
        "    seq = data.loc[data['Sequenza'] == id]\n",
        "\n",
        "    # Remove the unused columns.\n",
        "    seq = seq.drop(['Sequenza'], axis=1)\n",
        "    num_col = len(seq.columns)\n",
        "\n",
        "    # Iterate over each row of the selected sequence  \n",
        "    temp_row = []\n",
        "    for index, row in seq.iterrows():\n",
        "      temp_row = np.append(temp_row, row)\n",
        "    temp_row = np.reshape(temp_row, (-1, num_col))\n",
        "\n",
        "    temp.append(temp_row[:])\n",
        "\n",
        "  return temp, labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J8MI_pQGT4j"
      },
      "source": [
        "df_train, lbl_train = preprocessing_sequences(df_train, lbl_train)\n",
        "df_valid, lbl_valid = preprocessing_sequences(df_valid, lbl_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VvZMY3A1k3B"
      },
      "source": [
        "## 3.3) Create and manage the train and validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwpIhNo_v8ZI"
      },
      "source": [
        "ds_train = tf.data.Dataset.from_tensor_slices((df_train, lbl_train))\n",
        "ds_valid = tf.data.Dataset.from_tensor_slices((df_valid, lbl_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzAf41xJu1PL"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "BUFFER_SIZE = 5000\n",
        "random_seed = 1337"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmmKbrPZYibk"
      },
      "source": [
        "Function to apply some preprocessing when making batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bh-ogBsyccq"
      },
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE,seed=random_seed)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .prefetch(tf.data.AUTOTUNE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yug1V-wIpTqm"
      },
      "source": [
        "Now we divide in batches the validation and training sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYlfnP6jyfoR"
      },
      "source": [
        "train_batches = make_batches(ds_train)\n",
        "val_batches = make_batches(ds_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckT5WkCUpnEA"
      },
      "source": [
        "#4) Training Phase.\n",
        "\n",
        "In this section we organize all the operation needed to perform the train of the model and evaluate its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLeUdXBQToDq"
      },
      "source": [
        "## 4.1)Set the hyperparameters.\n",
        "Set the transformer hyperparameter, define the learning rate, optimizer and loss type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRVZLF5ETubo"
      },
      "source": [
        "# Model hyperparameters\n",
        "d_model = 32 # Dimension of the hidden representation.\n",
        "dim_feedforward = 256\n",
        "n_heads = 6\n",
        "num_layers = 3\n",
        "feat_dim = len(df.columns) - 3 # Number of feature inside each item of the sequence.\n",
        "\n",
        "# Changed.\n",
        "max_len=  len(df_train[0]) # Length of each sequence.\n",
        "\n",
        "# Parameter needed for separate classification from regression.\n",
        "# For now just regression is implemented.\n",
        "num_classes = 1\n",
        "\n",
        "# Network hyperparameter\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "optim_name = 'Adam'\n",
        "# tfa.optimizers.RectifiedAdam() seems to not work properly.\n",
        "\n",
        "# This loss and accuracy objects are meant for regression.\n",
        "# For classifications other metrics will be needed.\n",
        "loss_object = tf.keras.losses.MeanSquaredError()\n",
        "error_object = tf.keras.metrics.MeanAbsoluteError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3pxHFQVFwdd"
      },
      "source": [
        "## 4.2) Custom implementation of the loss and accuracy functions.\n",
        "\n",
        "Add a way to customize the loss and accuracy functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YInErpodyKFT"
      },
      "source": [
        "def loss_function(real,pred):\n",
        "\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  return loss_\n",
        "\n",
        "def error_function(real, pred):\n",
        "\n",
        "  accuracies = error_object(real, pred)\n",
        "  \n",
        "  return accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTr2DpFCY9Ft"
      },
      "source": [
        "Create the metric objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anzv8mRpISiF"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_error = tf.keras.metrics.Mean(name='train_error')\n",
        "val_error = tf.keras.metrics.Mean(name='validion_error')\n",
        "val_loss = tf.keras.metrics.Mean(name='validation_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_y-xc4IzDgu"
      },
      "source": [
        "## 4.4) Manage checkpoint and Tensorboard.\n",
        "Create the model and load last checkpoint if it exist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRgByXvatFtu"
      },
      "source": [
        "# Create the transformer.\n",
        "transformer = EncoderRegressor(d_model=d_model, dim_feedforward=dim_feedforward, n_heads=n_heads, num_layers=num_layers, feat_dim=feat_dim, max_len=max_len, num_classes = num_classes)\n",
        "\n",
        "# Checkpoint management.\n",
        "use_checkpoint = False\n",
        "if use_checkpoint:\n",
        "  ckpt = tf.train.Checkpoint(trasformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "  ckpt_manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_DIR, max_to_keep=5)\n",
        "  if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Restored latest checkpoint\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txv-PSFPY1w6"
      },
      "source": [
        "Set paths for tensorboard visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk3_uzV8Y06P"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = LOGS_DIR + '/gradient_tape/' + current_time + '/train'\n",
        "valid_log_dir = LOGS_DIR + '/gradient_tape/' + current_time + '/valid'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "valid_summary_writer = tf.summary.create_file_writer(valid_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIwzbH0zSW_S"
      },
      "source": [
        "### 4.4.1) Informations needed for the confusion matrix visualization inside tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcN9SLcRSWZU"
      },
      "source": [
        "# Needed for confusion matrix visualization\n",
        "# This can be done just because the regression task has a finite number of labels.\n",
        "tag_list = [float(_dummy) for _dummy in range(number_of_labels + 1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GDoZMJThM1p"
      },
      "source": [
        "Utility function needed for the confusion matrix visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcKblK8XhMJI"
      },
      "source": [
        "def plot_to_image(figure):\n",
        "  \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
        "  returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
        "  # Save the plot to a PNG in memory.\n",
        "  buf = io.BytesIO()\n",
        "  plt.savefig(buf, format='png')\n",
        "  # Closing the figure prevents it from being displayed directly inside\n",
        "  # the notebook.\n",
        "  plt.close(figure)\n",
        "  buf.seek(0)\n",
        "  # Convert PNG buffer to TF image\n",
        "  image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "  # Add the batch dimension\n",
        "  image = tf.expand_dims(image, 0)\n",
        "  return image\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "  \"\"\"\n",
        "  Returns a matplotlib figure containing the plotted confusion matrix.\n",
        "\n",
        "  Args:\n",
        "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
        "    class_names (array, shape = [n]): String names of the integer classes\n",
        "  \"\"\"\n",
        "  figure = plt.figure(figsize=(8, 8))\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(len(class_names))\n",
        "  plt.xticks(tick_marks, class_names, rotation=45)\n",
        "  plt.yticks(tick_marks, class_names)\n",
        "\n",
        "  # Compute the labels from the normalized confusion matrix.\n",
        "  # Remove number equals to zero.\n",
        "  divider = cm.sum(axis=1)[:, np.newaxis]\n",
        "  divider = np.where(divider!=0, divider, 1)\n",
        "\n",
        "  labels = np.around(cm.astype('float') / divider, decimals=2)\n",
        "\n",
        "  # Use white text if squares are dark; otherwise black.\n",
        "  threshold = cm.max() / 2.\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    color = \"white\" if cm[i, j] > threshold else \"black\"\n",
        "    plt.text(j, i, labels[i, j], horizontalalignment=\"center\", color=color)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  return figure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud5JnnWNGUk6"
      },
      "source": [
        "## 4.5) Train the model.\n",
        "Set the number of epoch and define the train step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkrNOWVTz_94"
      },
      "source": [
        "# Number of epochs\n",
        "EPOCHS = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHlsTQSds8eF"
      },
      "source": [
        "def valid_step(inp,tar):\n",
        "  tar_real = tar\n",
        "\n",
        "  \n",
        "  predictions = transformer(inp, training = False) \n",
        "  loss = loss_function(tar_real, predictions)\n",
        "  accuracy = error_function(tar_real, predictions)\n",
        "\n",
        "\n",
        "  val_loss(loss)\n",
        "  val_error(accuracy)\n",
        "  \n",
        "  return predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCoA_OUOYnlP"
      },
      "source": [
        "def train_step(inp,tar):\n",
        "  tar_real = tar\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = transformer(inp, training = True) \n",
        "    loss = loss_function(tar_real, predictions)\n",
        "    accuracy = error_function(tar_real, predictions)\n",
        "    \n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_error(accuracy)\n",
        "  \n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXFVuSG0YFhv"
      },
      "source": [
        "Start the training.\n",
        "\n",
        "The summary writers will save the prediction and ground truth distributions.\n",
        "They will save the loss and the error too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4n-a9GeZXLB"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  # Needed for histogram visualization.\n",
        "  predictions_histogram = []\n",
        "  labels_histogram = []\n",
        "  y_pred, y_true = [],[]\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_error.reset_states()\n",
        "\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "    predictions = train_step(inp, tar)\n",
        "    # Save the histogram of predictions.\n",
        "    predictions_histogram = np.hstack((predictions_histogram, tf.reshape(predictions, len(predictions))))    \n",
        "    labels_histogram = np.hstack((labels_histogram, tar))\n",
        "    # Data needed for confusion matrix\n",
        "    y_pred.extend(np.around(np.array(predictions)*number_of_labels))\n",
        "    y_true.extend(np.around(np.array(tar)*number_of_labels))\n",
        "\n",
        "  # Build confusion matrix.\n",
        "  cm = confusion_matrix(y_pred, y_true)\n",
        "  figure = plot_confusion_matrix(cm, class_names=tag_list)\n",
        "  cm_image = plot_to_image(figure)\n",
        "\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar('Loss', train_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('Error', train_error.result(), step=epoch)\n",
        "    tf.summary.histogram('Predictions distribution', predictions_histogram, step = epoch)\n",
        "    tf.summary.histogram('Ground Truth distribution', labels_histogram, step = epoch)\n",
        "    tf.summary.image('Confusion Matrix', cm_image, step=epoch)\n",
        "  \n",
        "  if epoch == EPOCHS - 1:\n",
        "    with train_summary_writer.as_default():\n",
        "      tf.summary.histogram('Predictions distribution last epoch', predictions_histogram, step = 0)\n",
        "      tf.summary.histogram('Ground Truth distribution last epoch', labels_histogram, step = 0)\n",
        "  \n",
        "  predictions_histogram = []\n",
        "  labels_histogram = []\n",
        "  val_loss.reset_states()\n",
        "  val_error.reset_states()\n",
        "\n",
        "  for (batch, (inp,tar)) in enumerate(val_batches):\n",
        "    predictions = valid_step(inp,tar)\n",
        "    # Save the histogram of predictions.\n",
        "    predictions_histogram = np.hstack((predictions_histogram, tf.reshape(predictions, len(predictions))))    \n",
        "    labels_histogram = np.hstack((labels_histogram, tar))\n",
        "\n",
        "  with valid_summary_writer.as_default():\n",
        "    tf.summary.scalar('Loss', val_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('Error', val_error.result(), step=epoch)\n",
        "    tf.summary.histogram('Predictions distribution', predictions_histogram, step = epoch)\n",
        "    tf.summary.histogram('Ground Truth distribution', labels_histogram, step = 0)\n",
        "  \n",
        "  if epoch == EPOCHS - 1:\n",
        "    with valid_summary_writer.as_default():\n",
        "      tf.summary.histogram('Predictions distribution last epoch', predictions_histogram, step = 0)\n",
        "      tf.summary.histogram('Ground Truth distribution last epoch', labels_histogram, step = 0)\n",
        "\n",
        "\n",
        "\n",
        "  if (epoch + 1) % 5 == 0 and use_checkpoint:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Error {train_error.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n",
        "\n",
        "if use_checkpoint:\n",
        "  print(ckpt_manager.save())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTYGPGv7YXTD"
      },
      "source": [
        "## 4.6) Save the model.\n",
        "Save the summary of the model on tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klOOBqzAR3nk"
      },
      "source": [
        "hyperparameters = [\n",
        "                   f'Dimension of the encoded representation: {d_model}',\n",
        "                   f'Dimension of the feedforward layers: {dim_feedforward}',\n",
        "                   f'Number of attention heads: {n_heads}',\n",
        "                   f'Number of encoding layers: {num_layers}',\n",
        "                   f'Number of starting features: {feat_dim}',\n",
        "                   f'Max_length of each sequence: {max_len}',\n",
        "                   f'Learning rate: {learning_rate}',\n",
        "                   f'Optimizer: {optim_name}'\n",
        "                   \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sWNU6wWqf7D"
      },
      "source": [
        "def get_summary_str(model):\n",
        "    lines = []\n",
        "    model.summary(print_fn=lines.append)\n",
        "    for item in hyperparameters:\n",
        "      lines.append(item)\n",
        "    # Add initial spaces to avoid markdown formatting in TensorBoard\n",
        "    return '    ' + '\\n    '.join(lines)\n",
        "\n",
        "# Add the summary as text in Tensorboard\n",
        "with train_summary_writer.as_default():\n",
        "  tf.summary.text('Model configuration', get_summary_str(transformer), step=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRWfGyznVOTG"
      },
      "source": [
        "Save the transformer model in .h5 format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DObxkZ5uwhUK"
      },
      "source": [
        "transformer.save(MODEL_DIR + '/' + current_time + '/transformers')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}