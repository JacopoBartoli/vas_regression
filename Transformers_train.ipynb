{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers_train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kfyIhFnIYUWo",
        "hcnuMP6rY-x4",
        "Q9RX5tAUnhGn"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacopoBartoli/vas_regression/blob/main/Transformers_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mr9qPI0vSKU"
      },
      "source": [
        "#1) Install packages and organize imports.\n",
        "In this section we install the needed packages and import them.\n",
        "We set some variables for the used paths, and mount GDrive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRCPE_6yYdDg",
        "outputId": "9fd89dd5-2871-4568-e939-bb36a9268ea6"
      },
      "source": [
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrMRVkjJE88R"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorboard\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import datetime\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfeT4MTUYo-Y"
      },
      "source": [
        "Save some usefull paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xugDyHkCZlbm"
      },
      "source": [
        "DATASET_DIR = '/content/gdrive/My Drive/IVA/data/'\n",
        "LOGS_DIR = '/content/gdrive/My Drive/IVA/logs'\n",
        "CHECKPOINT_DIR = '/content/gdrive/My Drive/IVA/checkpoint/train'\n",
        "MODEL_DIR = '/content/gdrive/My Drive/IVA/model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_tfWIAAxaPB"
      },
      "source": [
        "Mount the drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUIi6rICZcI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599ea36d-2a7a-4557-8915-fb113b6cee72"
      },
      "source": [
        "# Mount your drive to access the dataset.\n",
        "# Remember to link the dataset as explained above.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls -l \"/content/gdrive/My Drive/\"\n",
        "#!rm -rf './IVA/logs/gradient_tape/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "ls: '/content/gdrive/My Drive/DeepFashion2 Dataset': No such file or directory\n",
            "total 2794\n",
            "drwx------ 2 root root   4096 Mar 18 14:03  3D_result\n",
            "-rw------- 1 root root 555998 Dec  3  2020  7036362.pdf\n",
            "-rw------- 1 root root   6420 Dec  2  2019 '7036362-sol (1).pdf'\n",
            "-rw------- 1 root root   6669 Dec  3  2020  7036362-sol.pdf\n",
            "-rw------- 1 root root 281590 Dec  8  2020  bartoli_jacopo_report2.pdf\n",
            "drwx------ 2 root root   4096 Nov 28  2020 'Colab Notebooks'\n",
            "drwx------ 2 root root   4096 Dec  1  2020  DDM\n",
            "lrw------- 1 root root      0 Mar 24 15:34 'DeepFashion2 Dataset' -> '/content/gdrive/.shortcut-targets-by-id/125F48fsMBz2EF0Cpqk6aaHet5VH399Ok/DeepFashion2 Dataset'\n",
            "drwx------ 6 root root   4096 Sep  3 11:19  IVA\n",
            "-rw------- 1 root root 109584 Nov 10  2020  Jacopo_Bartoli_report.pdf\n",
            "lrw------- 1 root root     25 Nov 30  2020 'My Drive' -> '/content/gdrive/My Drive/'\n",
            "-rw------- 1 root root  94655 Dec  2  2019  PdS-magistrale-ambiti-2019-Bartoli-Jacopo-7036362.pdf\n",
            "-rw------- 1 root root  94655 Dec  2  2019  PdS-magistrale-ambiti-2019-Bartoli-Jacopo.pdf\n",
            "-rw------- 1 root root 442582 Mar 22 17:52  scan.png\n",
            "-rw------- 1 root root 734000 Mar 22  2019  tesi_bartoli_annotata_1.pdf\n",
            "-rw------- 1 root root 511579 Dec  2  2019  variazione_piano_di_studio_7036362.pdf\n",
            "drwx------ 2 root root   4096 Mar 24 21:43  VMR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmDLWWbEXxPA"
      },
      "source": [
        "#2) Define our transformer.\n",
        "In this section we implement our transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfyIhFnIYUWo"
      },
      "source": [
        "##2.1) Utility functions.\n",
        "Define some utilities functions, for the positional encodings and the feed forward network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqAs6b2vX9_a"
      },
      "source": [
        "# Define the positional encoding function.\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7H8TDWjBGKj"
      },
      "source": [
        "# Define the feed forward network\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "                              tf.keras.layers.Dense(dff, activation='relu'),\n",
        "                              tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcnuMP6rY-x4"
      },
      "source": [
        "##2.2) Define the encoder layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J15HQutS_-2T"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(num_heads, output_shape=d_model, key_dim=24)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    attn_output = self.mha(x,x,x,mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdmfhgDYZl84"
      },
      "source": [
        "##2.3) Define the encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQX5vJBXvOdl"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate ) for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef8kHLd9x6xN"
      },
      "source": [
        "## 2.4) Define the transformer with the regression layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oND3EggGFYGa"
      },
      "source": [
        "class EncoderRegressor(tf.keras.Model):\n",
        "  def __init__(self, feat_dim, max_len, d_model, n_heads, num_layers, dim_feedforward, num_classes=1, dropout=0.1, pos_encoding='fixed', activation='gelu', norm='BatchNorm'):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.max_len = max_len\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    \n",
        "    self.flatten_inp = tf.keras.layers.Flatten()\n",
        "\n",
        "    self.project_inp = tf.keras.layers.Dense(max_len*d_model)\n",
        "\n",
        "    self.reshape = tf.keras.layers.Reshape((max_len, d_model))\n",
        "\n",
        "    self.pos_encoding = positional_encoding(2048, self.d_model)\n",
        "\n",
        "\n",
        "    self.encoder_layer = Encoder(num_layers = num_layers, d_model = d_model, num_heads = n_heads, dff= dim_feedforward, rate=0.01)\n",
        "\n",
        "    self.act = tf.keras.activations.get(activation)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    \n",
        "    self.reg = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    self.feat_dim = feat_dim\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  def call(self, inputs, training):    \n",
        "      enc_padding_mask = None #create_padding_mask(inputs)\n",
        "      seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "      # Flatten the input tensor and map in a different vector space(d_model)\n",
        "      x = self.flatten_inp(inputs)\n",
        "      x = self.project_inp(x)\n",
        "\n",
        "      # Reshape the tensor to adapt the shape [batch_size, sequence_lenght, d_model]\n",
        "      x = self.reshape(x)\n",
        "\n",
        "      # Positional encoding.\n",
        "      x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "      x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "      \n",
        "      # Encoder Layer\n",
        "      x = self.encoder_layer(x, training, enc_padding_mask)\n",
        "      x = self.dropout(x, training=training)\n",
        "\n",
        "      # Regression Layers\n",
        "      x = self.flatten(x)\n",
        "      x = self.reg(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpZLk23okxQx"
      },
      "source": [
        "#3) Manage the data.\n",
        "In this section we manipulate and extract the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9RX5tAUnhGn"
      },
      "source": [
        "##3.1) Load the train set.\n",
        "Define the name of the dataset used for \n",
        "training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJDvwZo7DCvM"
      },
      "source": [
        "# Name of the dataset used.\n",
        "TRAIN_SET = 'train-vs.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaTqWROxJ2D1"
      },
      "source": [
        "Load the train set from a .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW3SLDKN-NBi",
        "outputId": "f349cd85-bc05-4eae-bdef-5199798c85e2"
      },
      "source": [
        "df = pd.read_csv(DATASET_DIR + TRAIN_SET)\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Sequenza  Frame      Vel1      Vel2  ...     Vel10     Vel11     Vel12  Label\n",
            "0         0      0  1.107498  0.802499  ...  1.191629  1.168866  1.850598      0\n",
            "1         0      1  0.091547  0.071286  ...  0.188025  0.507695  0.759535      0\n",
            "2         0      2  0.433357 -0.129341  ...  0.093921  0.048248  0.238178      0\n",
            "3         0      3  0.091547 -0.233620  ... -0.146718 -0.063956  0.029616      0\n",
            "4         0      4 -0.003324 -0.333743  ... -0.393765 -0.303028 -0.271540      0\n",
            "\n",
            "[5 rows x 15 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXXVYXCFKBCJ"
      },
      "source": [
        "## 3.2) Extract the labels from the dataset.\n",
        "Extract the train set column that contains the labels and group them by sequence id. In this way the output is a list of a single label for each sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlC1M1jSI77E"
      },
      "source": [
        "# Extract the column that contains the labels.\n",
        "lbl = df['Label']\n",
        "seq_ids = df['Sequenza']\n",
        "\n",
        "temp = pd.concat([seq_ids, lbl], axis=1)\n",
        "temp = temp.set_index('Sequenza')\n",
        "temp = temp.groupby(level='Sequenza').mean()\n",
        "\n",
        "lbl = temp['Label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJEegOzWKphi"
      },
      "source": [
        "We need to manipulate the dataframe in order to get the desired input format.\n",
        "In particular we want an array of the shape [num_sequence, sequence_elements, features_dim].\n",
        "To do this we will first remove the label column from the dataframe. \n",
        "The input sequeces in the dataframe need to be numbered from 0 to $num\\_seq - 1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19PkuhgsJTiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d47a90c-ceb3-40df-b4c8-9de66add3f93"
      },
      "source": [
        "# Drop the label column.\n",
        "data = df.drop(['Label'], axis = 1)\n",
        "num_seqs = data['Sequenza'].max() + 1\n",
        "\n",
        "# Create the new dataset.\n",
        "temp = []\n",
        "for id in tqdm(range(num_seqs)):\n",
        "  # Extract sequences one by one.\n",
        "  seq = data.loc[data['Sequenza'] == id]\n",
        "\n",
        "  # Remove the unused columns.\n",
        "  seq = seq.drop(['Sequenza','Frame'], axis=1)\n",
        "  num_col = len(seq.columns)\n",
        "\n",
        "  # Iterate over each row of the selected sequence  \n",
        "  temp_row = []\n",
        "  for index, row in seq.iterrows():\n",
        "    temp_row = np.append(temp_row, row)\n",
        "  temp_row = np.reshape(temp_row, (-1, num_col))\n",
        "\n",
        "  temp.append(temp_row[:])\n",
        "\n",
        "data = temp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 180/180 [00:03<00:00, 45.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvyvIxRZ119G"
      },
      "source": [
        "Then we need to pad the each sequence in order to make them all of the same lenght."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWHhPxz86t4c"
      },
      "source": [
        "# Default values:\n",
        "# Default value added to fill the sequence si 0\n",
        "# padding = 'pre' ---> The sequence will be filled inserting the zeros at the start.\n",
        "data = tf.keras.preprocessing.sequence.pad_sequences(data, dtype='float64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MpOI7Cfs9We",
        "outputId": "f5baa790-2c1e-4be8-eee9-f8f060de7470"
      },
      "source": [
        "print(len(data[100]))\n",
        "print(len(data[160]))\n",
        "print(data[100][675])\n",
        "print(data[160][641])\n",
        "print(data[160][11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "681\n",
            "681\n",
            "[5.69299421 3.39554945 2.37205529 4.94836284 0.95650173 1.60686903\n",
            " 1.93938091 2.44618251 3.04341193 2.94810158 5.81767736 5.8058954 ]\n",
            "[0.40584004 0.62164306 0.26023285 1.78844872 0.03775138 0.64564219\n",
            " 0.76591467 1.51821512 0.75974073 0.99427973 0.43091039 0.29837468]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VvZMY3A1k3B"
      },
      "source": [
        "## 3.3) Create and manage the train set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwpIhNo_v8ZI",
        "outputId": "86952d76-5a3b-4d58-d7b0-4c75b1028d1c"
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices((data, lbl))\n",
        "val_examples = ds.take(18)\n",
        "train_examples = ds.skip(18)\n",
        "print(val_examples)\n",
        "print(train_examples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<TakeDataset shapes: ((681, 12), ()), types: (tf.float64, tf.int64)>\n",
            "<SkipDataset shapes: ((681, 12), ()), types: (tf.float64, tf.int64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzAf41xJu1PL"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "BUFFER_SIZE = 180\n",
        "random_seed = 1337"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bh-ogBsyccq"
      },
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE,seed=random_seed)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .prefetch(tf.data.AUTOTUNE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yug1V-wIpTqm"
      },
      "source": [
        "Make batches for validation and training sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYlfnP6jyfoR"
      },
      "source": [
        "train_batches = make_batches(train_examples)\n",
        "val_batches = make_batches(val_examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckT5WkCUpnEA"
      },
      "source": [
        "#4) Training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLeUdXBQToDq"
      },
      "source": [
        "## 4.1)Set the hyperparameters.\n",
        "Set the transformer hyperparameter, define the learning rate, optimizer and loss type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRVZLF5ETubo"
      },
      "source": [
        "# Model hyperparameters\n",
        "d_model = 32\n",
        "dim_feedforward = 256\n",
        "n_heads = 6\n",
        "num_layers = 3\n",
        "feat_dim = 12 # Number of feature inside each item of the sequence.\n",
        "max_len=  681 # Lenght of each sequence.\n",
        "\n",
        "# Parameter needed for separate classification from regression.\n",
        "# For now just regression is implemented.\n",
        "num_classes = 1\n",
        "is_classification = False\n",
        "\n",
        "\n",
        "# Network hyperparameter\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "# tfa.optimizers.RectifiedAdam() seems to not work properly.\n",
        "\n",
        "# This loss and accuracy objects are meant for regression.\n",
        "# For classifications other metrics will be needed.\n",
        "loss_object = tf.keras.losses.MeanSquaredError()\n",
        "accuracy_object = tf.keras.metrics.MeanAbsoluteError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3pxHFQVFwdd"
      },
      "source": [
        "## 4.2) Custom implementation of the loss and accuracy functions.\n",
        "\n",
        "Add a way to customize the loss and accuracy functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YInErpodyKFT"
      },
      "source": [
        "def loss_function(real,pred):\n",
        "\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  return loss_\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "\n",
        "  accuracies = accuracy_object(real, pred)\n",
        "  \n",
        "  return accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTr2DpFCY9Ft"
      },
      "source": [
        "Create the metric objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anzv8mRpISiF"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_y-xc4IzDgu"
      },
      "source": [
        "## 4.4) Manage checkpoint and Tensorboard.\n",
        "Create the model and load last checkpoint if it exist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRgByXvatFtu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7191e39-65d6-4254-cc4b-140cbd142282"
      },
      "source": [
        "# Create the transformer.\n",
        "transformer = EncoderRegressor(d_model=d_model, dim_feedforward=dim_feedforward, n_heads=n_heads, num_layers=num_layers, feat_dim=feat_dim, max_len=max_len, num_classes = num_classes)\n",
        "\n",
        "# Checkpoint management.\n",
        "ckpt = tf.train.Checkpoint(trasformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_DIR, max_to_keep=5)\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print(\"Restored latest checkpoint\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restored latest checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txv-PSFPY1w6"
      },
      "source": [
        "Set paths for tensorboard visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk3_uzV8Y06P"
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = LOGS_DIR + '/gradient_tape/' + current_time + '/train'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud5JnnWNGUk6"
      },
      "source": [
        "## 4.5) Train the model.\n",
        "Set the number of epoch and define the train step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkrNOWVTz_94"
      },
      "source": [
        "# Number of epochs\n",
        "EPOCHS = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCoA_OUOYnlP"
      },
      "source": [
        "def train_step(inp,tar):\n",
        "  tar_real = tar\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = transformer(inp, training = True) \n",
        "    loss = loss_function(tar_real, predictions)\n",
        "    accuracy = accuracy_function(tar_real, predictions)\n",
        "    \n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(accuracy)\n",
        "  \n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXFVuSG0YFhv"
      },
      "source": [
        "Start the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4n-a9GeZXLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f4e48eb-eea5-4f5c-ceb9-761b1a5ddc79"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  # Needed for histogram visualization.\n",
        "  predictions_histogram = []\n",
        "  labels_histogram = []\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "    predictions = train_step(inp, tar)\n",
        "    # Save the histogram of predictions.\n",
        "    predictions_histogram = np.hstack((predictions_histogram, tf.reshape(predictions, len(predictions))))    \n",
        "    labels_histogram = np.hstack((labels_histogram, tar))\n",
        "\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    tf.summary.histogram('predictions distribution', predictions_histogram, step = epoch)\n",
        "    tf.summary.histogram('label distribution', labels_histogram, step = epoch)\n",
        "\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n",
        "\n",
        "print(ckpt_manager.save())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss 18.8808 Accuracy 3.4882\n",
            "Time taken for 1 epoch: 89.18 secs\n",
            "\n",
            "Epoch 2 Loss 7.9235 Accuracy 2.5802\n",
            "Time taken for 1 epoch: 81.93 secs\n",
            "\n",
            "Epoch 3 Loss 6.6498 Accuracy 2.4142\n",
            "Time taken for 1 epoch: 76.12 secs\n",
            "\n",
            "Epoch 4 Loss 4.7124 Accuracy 2.2737\n",
            "Time taken for 1 epoch: 69.44 secs\n",
            "\n",
            "Saving checkpoint for epoch 5 at /content/gdrive/My Drive/IVA/checkpoint/train/ckpt-8\n",
            "Epoch 5 Loss 4.2108 Accuracy 2.1474\n",
            "Time taken for 1 epoch: 99.28 secs\n",
            "\n",
            "Epoch 6 Loss 4.0342 Accuracy 2.0632\n",
            "Time taken for 1 epoch: 141.92 secs\n",
            "\n",
            "Epoch 7 Loss 2.4834 Accuracy 1.9575\n",
            "Time taken for 1 epoch: 80.35 secs\n",
            "\n",
            "Epoch 8 Loss 3.0392 Accuracy 1.8784\n",
            "Time taken for 1 epoch: 72.76 secs\n",
            "\n",
            "Epoch 9 Loss 2.8054 Accuracy 1.8167\n",
            "Time taken for 1 epoch: 141.93 secs\n",
            "\n",
            "Saving checkpoint for epoch 10 at /content/gdrive/My Drive/IVA/checkpoint/train/ckpt-9\n",
            "Epoch 10 Loss 3.8798 Accuracy 1.7882\n",
            "Time taken for 1 epoch: 94.91 secs\n",
            "\n",
            "Epoch 11 Loss 2.2984 Accuracy 1.7553\n",
            "Time taken for 1 epoch: 81.93 secs\n",
            "\n",
            "Epoch 12 Loss 1.6429 Accuracy 1.6962\n",
            "Time taken for 1 epoch: 81.93 secs\n",
            "\n",
            "Epoch 13 Loss 1.2432 Accuracy 1.6370\n",
            "Time taken for 1 epoch: 76.02 secs\n",
            "\n",
            "Epoch 14 Loss 0.8708 Accuracy 1.5734\n",
            "Time taken for 1 epoch: 76.05 secs\n",
            "\n",
            "Saving checkpoint for epoch 15 at /content/gdrive/My Drive/IVA/checkpoint/train/ckpt-10\n",
            "Epoch 15 Loss 0.7693 Accuracy 1.5116\n",
            "Time taken for 1 epoch: 94.32 secs\n",
            "\n",
            "Epoch 16 Loss 0.6819 Accuracy 1.4579\n",
            "Time taken for 1 epoch: 81.93 secs\n",
            "\n",
            "Epoch 17 Loss 0.6664 Accuracy 1.4093\n",
            "Time taken for 1 epoch: 73.68 secs\n",
            "\n",
            "Epoch 18 Loss 0.4373 Accuracy 1.3637\n",
            "Time taken for 1 epoch: 72.07 secs\n",
            "\n",
            "Epoch 19 Loss 0.5987 Accuracy 1.3209\n",
            "Time taken for 1 epoch: 67.92 secs\n",
            "\n",
            "Saving checkpoint for epoch 20 at /content/gdrive/My Drive/IVA/checkpoint/train/ckpt-11\n",
            "Epoch 20 Loss 0.5722 Accuracy 1.2823\n",
            "Time taken for 1 epoch: 83.83 secs\n",
            "\n",
            "Epoch 21 Loss 0.6178 Accuracy 1.2506\n",
            "Time taken for 1 epoch: 66.73 secs\n",
            "\n",
            "Epoch 22 Loss 0.5863 Accuracy 1.2204\n",
            "Time taken for 1 epoch: 81.93 secs\n",
            "\n",
            "Epoch 23 Loss 0.5761 Accuracy 1.1928\n",
            "Time taken for 1 epoch: 71.63 secs\n",
            "\n",
            "Epoch 24 Loss 0.6364 Accuracy 1.1690\n",
            "Time taken for 1 epoch: 74.50 secs\n",
            "\n",
            "Saving checkpoint for epoch 25 at /content/gdrive/My Drive/IVA/checkpoint/train/ckpt-12\n",
            "Epoch 25 Loss 0.5647 Accuracy 1.1459\n",
            "Time taken for 1 epoch: 100.19 secs\n",
            "\n",
            "Epoch 26 Loss 0.7255 Accuracy 1.1264\n",
            "Time taken for 1 epoch: 68.76 secs\n",
            "\n",
            "Epoch 27 Loss 0.4371 Accuracy 1.1053\n",
            "Time taken for 1 epoch: 65.90 secs\n",
            "\n",
            "Epoch 28 Loss 0.4366 Accuracy 1.0850\n",
            "Time taken for 1 epoch: 70.02 secs\n",
            "\n",
            "Epoch 29 Loss 0.5567 Accuracy 1.0673\n",
            "Time taken for 1 epoch: 65.28 secs\n",
            "\n",
            "Saving checkpoint for epoch 30 at /content/gdrive/My Drive/IVA/checkpoint/train/ckpt-13\n",
            "Epoch 30 Loss 0.7003 Accuracy 1.0513\n",
            "Time taken for 1 epoch: 82.44 secs\n",
            "\n",
            "/content/gdrive/My Drive/IVA/checkpoint/train/ckpt-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTYGPGv7YXTD"
      },
      "source": [
        "## 4.6) Save the model.\n",
        "Save the summary of the model on tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sWNU6wWqf7D"
      },
      "source": [
        "def get_summary_str(model):\n",
        "    lines = []\n",
        "    model.summary(print_fn=lines.append)\n",
        "    # Add initial spaces to avoid markdown formatting in TensorBoard\n",
        "    return '    ' + '\\n    '.join(lines)\n",
        "\n",
        "# Add the summary as text in Tensorboard\n",
        "with train_summary_writer.as_default():\n",
        "  tf.summary.text('Model configuration', get_summary_str(transformer), step=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6djv_C7RypI1"
      },
      "source": [
        "class ExportTransformer(tf.Module):\n",
        "  def __init__(self, transformer):\n",
        "    self.transformer = transformer\n",
        "    \n",
        "  @tf.function()\n",
        "  def __call__(self, inputs):\n",
        "    result = self.transformer(inputs, training = False)\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRWfGyznVOTG"
      },
      "source": [
        "Save the transformer model in .h5 format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DObxkZ5uwhUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5531eb-cbb2-4186-be96-29c093379095"
      },
      "source": [
        "# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\n",
        "# Currently not working\n",
        "exporter = ExportTransformer(transformer)\n",
        "tf.saved_model.save(exporter, export_dir=MODEL_DIR + '/' + current_time + '/transformers')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as __call__, dropout_6_layer_call_fn, dropout_6_layer_call_and_return_conditional_losses, encoder_layer_layer_call_fn, encoder_layer_layer_call_and_return_conditional_losses while saving (showing 5 of 186). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/IVA/model/20210916-102654/transformers/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/IVA/model/20210916-102654/transformers/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb3HGv1E6zgY"
      },
      "source": [
        "# 5) Manage the test data.\n",
        "In this section we manipulate and extract the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbAzjX7b6zgg"
      },
      "source": [
        "## 5.1) Load the test set.\n",
        "Define the name of the dataset used for testing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a9fw1cs6zgg"
      },
      "source": [
        "# Name of the dataset used.\n",
        "TEST_SET = 'test-vs.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX-IPSMKxrZR"
      },
      "source": [
        "Load the test set from a .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZU7MgkFx9ty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "737afbcc-d7d1-4250-98e4-ae5b0b17d863"
      },
      "source": [
        "df = pd.read_csv(DATASET_DIR + TEST_SET)\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Sequenza  Frame      Vel1      Vel2  ...     Vel10     Vel11     Vel12  Label\n",
            "0       180      0 -0.385031 -0.398413  ... -0.037925 -0.357710 -0.113936      0\n",
            "1       180      1 -0.341554 -0.266994  ... -0.428443 -0.271442 -0.195584      0\n",
            "2       180      2 -0.003324 -0.100123  ...  1.188008  0.627858 -0.336879      0\n",
            "3       180      3 -0.409479 -0.359790  ... -0.052292 -0.002093 -0.355900      0\n",
            "4       180      4 -0.061690 -0.019474  ... -0.309205 -0.175785 -0.271540      0\n",
            "\n",
            "[5 rows x 15 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h18WHv0H6zgg"
      },
      "source": [
        "## 5.2) Extract the labels from the dataset.\n",
        "Extract the test set column that contains the labels and group them by sequence id. In this way the output is a list of a single label for each sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTkQINQUyu3u"
      },
      "source": [
        "# Extract the column that contains the test labels.\n",
        "lbl = df['Label']\n",
        "seq_ids = df['Sequenza']\n",
        "\n",
        "temp = pd.concat([seq_ids, lbl], axis=1)\n",
        "temp = temp.set_index('Sequenza')\n",
        "temp = temp.groupby(level='Sequenza').mean()\n",
        "\n",
        "lbl_test = temp['Label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU_Q_mHw6zgh"
      },
      "source": [
        "Remove the label column from the dataframe, and transform the data in the correct input format. The sequences need to be numbered from 0 to $num\\_seq - 1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X30uq2906zgh",
        "outputId": "ce1ffaa2-f55c-434c-b35c-59b702653652"
      },
      "source": [
        "# Drop the label column.\n",
        "data = df.drop(['Label'], axis = 1)\n",
        "min_seq = data['Sequenza'].min()\n",
        "num_seqs = data['Sequenza'].max() - data['Sequenza'].min() + 1\n",
        "\n",
        "# Create the new dataset.\n",
        "temp = []\n",
        "for id in tqdm(range(min_seq, min_seq + num_seqs)):\n",
        "  # Extract sequences one by one.\n",
        "  seq = data.loc[data['Sequenza'] == id]\n",
        "\n",
        "  # Remove the unused columns.\n",
        "  seq = seq.drop(['Sequenza','Frame'], axis=1)\n",
        "  num_col = len(seq.columns)\n",
        "\n",
        "  # Iterate over each row of the selected sequence  \n",
        "  temp_row = []\n",
        "  for index, row in seq.iterrows():\n",
        "    temp_row = np.append(temp_row, row)\n",
        "  temp_row = np.reshape(temp_row, (-1, num_col))\n",
        "\n",
        "  temp.append(temp_row[:])\n",
        "\n",
        "data = temp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 49.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB5E4EaX6zgh"
      },
      "source": [
        "data = tf.keras.preprocessing.sequence.pad_sequences(data, maxlen = 681, dtype='float64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lVaoncb6zgh",
        "outputId": "4981611f-75f5-478f-8d02-b152695bed4d"
      },
      "source": [
        "print(len(data[19]))\n",
        "print(len(data[1]))\n",
        "print(data[19][680])\n",
        "#print(data[19][641])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "681\n",
            "681\n",
            "[1.26310275 0.94694645 0.72443641 0.16871895 0.40214158 0.80335244\n",
            " 0.08617381 0.29341975 1.53734696 1.27304387 1.21974766 1.38796965]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xw2jkzMK4eO"
      },
      "source": [
        "## 5.3) Create and manage the test set.\n",
        "Transform the dataset ina tensorflow.data.Dataset format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmSCiVFq6zgh"
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices((data, lbl_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43Zci5c-uVOu"
      },
      "source": [
        "Define batch size and other variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUBcOl2JuUOi"
      },
      "source": [
        "BATCH_SIZE = 1\n",
        "BUFFER_SIZE = 180\n",
        "random_seed = 1337"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1oankNfuZUP"
      },
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE,seed=random_seed)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .prefetch(tf.data.AUTOTUNE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmrpLHucubo0"
      },
      "source": [
        "test_batches = make_batches(ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LejAIrj65au"
      },
      "source": [
        "#5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6CpwcOx4rFU"
      },
      "source": [
        "error_object = tf.keras.metrics.MeanAbsoluteError()\n",
        "test_accuracy = tf.keras.metrics.Mean(name='test_accuracy')\n",
        "test_log_dir = LOGS_DIR + '/gradient_tape/' + current_time + '/test'\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_F3wENg488M"
      },
      "source": [
        "def accuracy_function(real, pred):\n",
        "\n",
        "  accuracies = error_object(real, pred)\n",
        "  \n",
        "  return accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6dl7iqm4_jM"
      },
      "source": [
        "start = time.time()\n",
        "  \n",
        "test_accuracy.reset_states()\n",
        "\n",
        "# Needed for histogram visualization.\n",
        "predictions_histogram = []\n",
        "labels_histogram = []\n",
        "\n",
        "\n",
        "for (batch, (inp, tar)) in enumerate(test_batches):\n",
        "    predictions = transformer(inp, training = False)\n",
        "    \n",
        "    accuracy = accuracy_function(tar, predictions)\n",
        "    # Save the histogram of predictions.\n",
        "    predictions_histogram = np.hstack((predictions_histogram, tf.reshape(predictions, len(predictions))))    \n",
        "    labels_histogram = np.hstack((labels_histogram, tar))\n",
        "    with test_summary_writer.as_default():\n",
        "      tf.summary.scalar('error', accuracy, step = batch)\n",
        "with test_summary_writer.as_default():\n",
        "   tf.summary.histogram('predictions distribution', predictions_histogram,step = 0)\n",
        "   tf.summary.histogram('label distribution', labels_histogram, step = 0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}